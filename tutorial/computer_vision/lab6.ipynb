{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab6.ipynb","provenance":[{"file_id":"https://github.com/spmallick/learnopencv/blob/master/PyTorch-Segmentation-torchvision/intro-seg.ipynb","timestamp":1635126597366}],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KMdbwOEbeQ0s"},"source":["# CSCI 3343 Lab 6: Pytorch for Image Prediction and Object Segmentation\n","\n","**Posted:** Monday, October 25, 2021\n","\n","**Due:** N/A\n","\n","__Total Points__: 0.5 (extra pts for the final grade)\n","\n","__Name__:\n","[Your first name] [Your last name], [Your BC username]\n","\n","(e.g. Donglai Wei, weidf)\n","\n","__Submission__: please rename the .ipynb file as __\\<your_username\\>_lab6.ipynb__ before you submit it to canvas. Example: weidf_lab6.ipynb."]},{"cell_type":"markdown","metadata":{"id":"8L-nWlkceQ06"},"source":["# Part 1. Semantic Segmentation in PyTorch (Lecture 15)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_mQVy39SeQ1B"},"source":["Here are some common use cases for the Semantic Segmentation:\n","\n","1. **Autonomous Driving**\n","\n","  <img src=\"https://cdn-images-1.medium.com/max/1600/1*JKmS08bllQ8SCajIPyiBBQ.png\" width=\"400\"/> <br/>\n","  <small> Source: CityScapes Dataset </small>\n","  \n","  In autonomous driving, the image which comes in from the camera is semantically segmented, thus each pixel in the image is classified\n","  into a class. This helps the computer understand what is present in the its surroundings and thus helps the car act accordingly.\n","\n","\n","2. **Facial Segmentation**\n","\n","  <img src=\"https://i.ytimg.com/vi/vrvwfFej_r4/maxresdefault.jpg\" width=\"400\"/> <br/>\n","  <small> Source: https://github.com/massimomauro/FASSEG-repository/blob/master/papers/multiclass_face_segmentation_ICIP2015.pdf </small>\n","\n","  Facial Segmentation is used for segmenting each part of the face into a category, like lips, eyes etc. This technique is used for\n","  many purposes such as gender estimation, age estimation, facial expression analysis, emotional analysis and more.\n","  \n","\n","3. **Indoor Object Segmentation**\n","\n","  <img src=\"https://cs.nyu.edu/~silberman/rmrc2014/header_semantic_segmentation.jpg\" width=\"400\"/><br/>\n","  <small> Source: http://buildingparser.stanford.edu/dataset.html </small>\n","\n","  Guess where is this used? In AR (Augmented Reality) and VR (Virtual Reality). AR applications when required segments the entire indoor area to understand where there \n","  are chairs, tables, people, wall, and other obstacles and so on.\n"," \n","\n","4. **Geo-Land Sensing**\n","\n","  <img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0924271616305305-fx1_lrg.jpg\" width=\"400\"/> <br/>\n","  <small> Source: https://www.sciencedirect.com/science/article/pii/S0924271616305305 </small>\n","\n","  Geo Land Sensing is a way of categorizing each pixel in satellite images into a category such that we can track the land cover of each\n","  area. So, say in some area there is a heavy deforestation taking place then appropriate measures can be taken.\n"]},{"cell_type":"markdown","metadata":{"id":"ZsIngeXleQ1H"},"source":["## (a) FCN: Fully convolutional network (FCN)\n","\n","**Acknowledgement**: `\n","Satya Mallick: https://github.com/spmallick/learnopencv`"]},{"cell_type":"markdown","metadata":{"id":"IZr_qTiRKOAL"},"source":["### (1a) Data download and visualization"]},{"cell_type":"code","metadata":{"id":"E75tGBh8KOWE"},"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","!wget -nv \"https://www.learnopencv.com/wp-content/uploads/2021/01/person-segmentation.jpeg\" -O person.png\n","img = Image.open('./person.png')\n","height=480\n","img = img.resize((int(float(img.size[0])*height/float(img.size[1])),height))\n","\n","plt.rcParams[\"figure.figsize\"] = (5,5)\n","plt.imshow(img); plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QB5-l7h4KnnR"},"source":["### (1b) Data pre-process\n","\n","- Convert it to Tensor - all the values in the image becomes between `[0, 1]` from `[0, 255]`\n","- Normalize it with the Imagenet specific values `mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]`\n","\n","And lastly, we unsqueeze the image so that it becomes `[1 x C x H x W]` from `[C x H x W]` <br/>\n","We need a batch dimension while passing it to the models."]},{"cell_type":"code","metadata":{"id":"6hvjw6nXKsvz"},"source":["# Apply the transformations needed\n","import torchvision.transforms as T\n","trf = T.Compose([T.ToTensor(), \n","                 T.Normalize(mean = [0.485, 0.456, 0.406], \n","                             std = [0.229, 0.224, 0.225])])\n","img_pt = trf(img).unsqueeze(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8BNGcmYSUwFM"},"source":["### (2) Model examination"]},{"cell_type":"code","metadata":{"id":"jebN_lm9eQ1W"},"source":["import torch\n","from torchvision import models\n","fcn = models.segmentation.fcn_resnet50(pretrained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"shnC_YQLeQ1v"},"source":["fcn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BhUMhvU4eQ2H"},"source":["### (3) Inference"]},{"cell_type":"code","metadata":{"id":"Ds-TiaXwlzrG"},"source":["# turn on the evaluation mode\n","fcn.eval()\n","\n","# only the deep learning part\n","with torch.no_grad():\n","  backbone_output = fcn.backbone(img_pt)\n","print('Network output shape:', backbone_output['out'].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QDujuCbeQ2h"},"source":["# the base model forward() has interpolation\n","# https://github.com/pytorch/vision/blob/9ae833af31a20e3a5113bfca30dc34ac708000d8/torchvision/models/segmentation/_utils.py#L27\n","with torch.no_grad():\n","  out = fcn(img_pt)['out']\n","print('Final output shape:', out.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xn3jqtH_eQ27"},"source":["### (4) Visualization\n","The model was trained on `21` classes and thus our output have `21` channels!\n","We take a max index for each pixel position, which represents the class"]},{"cell_type":"code","metadata":{"id":"5GA_GNohUHnR"},"source":["out_pred = out[0].argmax(0)\n","\n","# create a color pallette, selecting a color for each class\n","palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n","colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n","colors = (colors % 255).numpy().astype(\"uint8\")\n","\n","# plot the semantic segmentation predictions of 21 classes in each color\n","r = Image.fromarray(out_pred.byte().cpu().numpy()).resize(img.size)\n","r.putpalette(colors)\n","\n","import matplotlib.pyplot as plt\n","from skimage.color import label2rgb\n","import numpy as np\n","\n","plt.rcParams[\"figure.figsize\"] = (30,10)\n","plt.subplot(131)\n","plt.imshow(img);plt.axis('equal');plt.axis('off')\n","plt.subplot(132)\n","plt.imshow(r);plt.axis('equal');plt.axis('off')\n","plt.subplot(133)\n","plt.imshow(label2rgb(out_pred.cpu().numpy(), image=np.array(img)));plt.axis('equal');plt.axis('off')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdRkeAP0tU9d"},"source":["## (b) U-Net"]},{"cell_type":"markdown","metadata":{"id":"X3GAX9qIruHv"},"source":["### (1a) Data download and visualization"]},{"cell_type":"code","metadata":{"id":"ZxtABliMeQ5A"},"source":["# Download a brain image\n","! wget https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/TCGA_CS_4944.png -O  brain.png\n","\n","from PIL import Image\n","img = Image.open('brain.png')\n","\n","plt.rcParams[\"figure.figsize\"] = (5,5)\n","plt.imshow(img); plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1T7zhyl3sF4q"},"source":["### (Exercise 1a) (1b) Data pre-process"]},{"cell_type":"code","metadata":{"id":"tB8fKk2LsGAA"},"source":["import numpy as np\n","from torchvision import transforms\n","\n","#### TODO: compute channel-wise mean and std\n","# need to feed it to: transforms.Normalize\n","noramlization_mean = ????\n","noramlization_std = ????\n","\n","preprocess = transforms.Compose([\n","    #### TODO: compute channel-wise mean and std\n","    ????\n","])\n","\n","# TODO: pre-process the image and make it 4-dimensional\n","input_batch = ????"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zway1Zw6runF"},"source":["### (2) Model examination"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hCHtbO0gru2W","executionInfo":{"status":"ok","timestamp":1635137094803,"user_tz":240,"elapsed":272,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"8b31a316-17cf-42df-85af-4c0f80b0c7d4"},"source":["import torch\n","unet = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n","    in_channels=3, out_channels=1, pretrained=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"]}]},{"cell_type":"markdown","metadata":{"id":"2I2O37ErsNUp"},"source":["### (Exercise 1b) (3) Inference"]},{"cell_type":"code","metadata":{"id":"ox37hPKVsNch"},"source":["#### TODO:\n","output_unet = ????"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PvhSgC_stDu"},"source":["### (4) Visualization"]},{"cell_type":"code","metadata":{"id":"SI0Q9iQqstMh"},"source":["out_pred = output_unet[0,0]\n","\n","plt.rcParams[\"figure.figsize\"] = (30,10)\n","plt.subplot(131)\n","plt.imshow(img);plt.axis('equal');plt.axis('off')\n","plt.subplot(132)\n","plt.imshow(label2rgb(out_pred.cpu().numpy()));plt.axis('equal');plt.axis('off')\n","plt.subplot(133)\n","plt.imshow(label2rgb(out_pred.cpu().numpy(), image=np.array(img)));plt.axis('equal');plt.axis('off')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-S5FO4QP1j9S"},"source":["# Part 2. Image generation (Lecture 16)"]},{"cell_type":"markdown","metadata":{"id":"echRHrLb1mLB"},"source":["## (a) Image synthesis: GAN (DCGAN)\n","\n","**Acknowledgement**: `Nathan Inkawhich <https://github.com/inkawhich>`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGJP6JKb2ik6","executionInfo":{"status":"ok","timestamp":1635138703022,"user_tz":240,"elapsed":141,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"04342f29-d136-48b3-d5c6-129f6b917f33"},"source":["from __future__ import print_function\n","#%matplotlib inline\n","import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","\n","# Set random seem for reproducibility\n","manualSeed = 999\n","#manualSeed = random.randint(1, 10000) # use if you want new results\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Seed:  999\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f6f729d39d0>"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"txVm7Ay22uFa"},"source":["### (1a) Data download and pre-process"]},{"cell_type":"code","metadata":{"id":"iGGjVTMQ2uRx"},"source":["import os\n","import zipfile \n","import gdown\n","import torch\n","from natsort import natsorted\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","\n","## Setup\n","# Number of gpus available\n","ngpu = 1\n","device = torch.device('cuda:0' if (\n","    torch.cuda.is_available() and ngpu > 0) else 'cpu')\n","\n","## Fetch data from Google Drive \n","# Root directory for the dataset\n","data_root = './'\n","# Path to folder with the dataset\n","dataset_folder = f'{data_root}/img_align_celeba'\n","# URL for the CelebA dataset\n","url = 'https://drive.google.com/uc?id=1cNIac61PSA_LqDFYFUeyaQYekYPc75NH'\n","# Path to download the dataset to\n","download_path = f'{data_root}/img_align_celeba.zip'\n","\n","# Create required directories \n","if not os.path.exists(data_root):\n","  os.makedirs(data_root)\n","  os.makedirs(dataset_folder)\n","\n","# Download the dataset from google drive\n","gdown.download(url, download_path, quiet=False)\n","\n","# Unzip the downloaded file \n","with zipfile.ZipFile(download_path, 'r') as ziphandler:\n","  ziphandler.extractall(dataset_folder)\n","\n","## Create a custom Dataset class\n","class CelebADataset(Dataset):\n","  def __init__(self, root_dir, transform=None):\n","    \"\"\"\n","    Args:\n","      root_dir (string): Directory with all the images\n","      transform (callable, optional): transform to be applied to each image sample\n","    \"\"\"\n","    # Read names of images in the root directory\n","    image_names = os.listdir(root_dir)\n","\n","    self.root_dir = root_dir\n","    self.transform = transform \n","    self.image_names = natsorted(image_names)\n","\n","  def __len__(self): \n","    return len(self.image_names)\n","\n","  def __getitem__(self, idx):\n","    # Get the path to the image \n","    img_path = os.path.join(self.root_dir, self.image_names[idx])\n","    # Load image and convert it to RGB\n","    img = Image.open(img_path).convert('RGB')\n","    # Apply transformations to the image\n","    if self.transform:\n","      img = self.transform(img)\n","\n","    return img\n","\n","## Load the dataset \n","# Path to directory with all the images\n","img_folder = f'{dataset_folder}/img_align_celeba'\n","# Spatial size of training images, images are resized to this size.\n","image_size = 64\n","# Transformations to be applied to each individual image sample\n","transform=transforms.Compose([\n","    transforms.Resize(image_size),\n","    transforms.CenterCrop(image_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                          std=[0.5, 0.5, 0.5])\n","])\n","# Load the dataset from file and apply transformations\n","celeba_dataset = CelebADataset(img_folder, transform)\n","\n","## Create a dataloader \n","# Batch size during training\n","batch_size = 128\n","# Number of workers for the dataloader\n","num_workers = 0 if device.type == 'cuda' else 1\n","# Whether to put fetched data tensors to pinned memory\n","pin_memory = True if device.type == 'cuda' else False\n","\n","celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,\n","                                                batch_size=batch_size,\n","                                                num_workers=num_workers,\n","                                                pin_memory=pin_memory,\n","                                                shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xa3v0p-fK0SQ"},"source":["### (1b) Data visualization"]},{"cell_type":"code","metadata":{"id":"RACTGq7p4qjg"},"source":["# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","\n","# Plot some training images\n","real_batch = next(iter(celeba_dataloader))\n","plt.figure(figsize=(10,10))\n","plt.axis(\"off\")\n","plt.title(\"Training Images\")\n","plt.imshow(np.transpose(vutils.make_grid(real_batch.to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wUn5N_ZJ2mCZ"},"source":["### (2a) Model definition"]},{"cell_type":"code","metadata":{"id":"p2sg1W6E5a_r"},"source":["# Generator Code\n","\n","class Generator(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Generator, self).__init__()\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","            # input is Z, going into a convolution\n","            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            # state size. (ngf*8) x 4 x 4\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            # state size. (ngf*4) x 8 x 8\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            # state size. (ngf*2) x 16 x 16\n","            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            # state size. (ngf) x 32 x 32\n","            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","            # state size. (nc) x 64 x 64\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EjntWz25YNF"},"source":["class Discriminator(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Discriminator, self).__init__()\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","            # input is (nc) x 64 x 64\n","            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf) x 32 x 32\n","            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*2) x 16 x 16\n","            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*4) x 8 x 8\n","            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*8) x 4 x 4\n","            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaVLFe17LBvk"},"source":["### (2b) Model Initialization"]},{"cell_type":"code","metadata":{"id":"pPwgJyly2mLM"},"source":["# Batch size during training\n","batch_size = 128\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","# Size of feature maps in generator\n","ngf = 64\n","# Size of feature maps in discriminator\n","ndf = 64\n","# Number of training epochs\n","num_epochs = 5\n","# Learning rate for optimizers\n","lr = 0.0002\n","# Beta1 hyperparam for Adam optimizers\n","beta1 = 0.5\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUfunGki5gcf"},"source":["# Create the generator\n","netG = Generator(ngpu).to(device)\n","netD = Discriminator(ngpu).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHE_NNwwLPrT"},"source":["### (3) Loss function"]},{"cell_type":"code","metadata":{"id":"NCyqkF4g5u71"},"source":["# Initialize BCELoss function\n","criterion = nn.BCELoss()\n","\n","# Create batch of latent vectors that we will use to visualize\n","#  the progression of the generator\n","fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n","\n","# Establish convention for real and fake labels during training\n","real_label = 1\n","fake_label = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wfyq0St0LSzb"},"source":["### (4) Optimization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oMX32vv6T8t","executionInfo":{"status":"ok","timestamp":1635139565466,"user_tz":240,"elapsed":118,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"7ed594d4-1513-436d-8a50-8304b7c944e0"},"source":["\n","# Setup Adam optimizers for both G and D\n","optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":98}]},{"cell_type":"markdown","metadata":{"id":"VENW03MPLX9h"},"source":["### (Excercise 2) (5) Start Training "]},{"cell_type":"code","metadata":{"id":"f0nkU94G5xXD"},"source":["# Training Loop\n","\n","# Lists to keep track of progress\n","img_list = []\n","G_losses = []\n","D_losses = []\n","iters = 0\n","\n","plt.figure(figsize=(10,10))\n","print(\"Starting Training Loop...\")\n","# For each epoch\n","for epoch in range(num_epochs):\n","    # For each batch in the dataloader\n","    for i, data in enumerate(celeba_dataloader, 0):\n","        \n","        ############################\n","        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","        ###########################\n","        ## Train with all-real batch\n","        netD.zero_grad()\n","        # Format batch\n","        real_cpu = data.to(device)\n","        b_size = real_cpu.size(0)\n","        label = torch.full((b_size,), 0, device=device).type(torch.float32)\n","        \n","        \n","        # Forward pass real batch through D\n","        output = netD(real_cpu).view(-1)\n","        # Calculate loss on all-real batch\n","        \n","        #### TODO \n","        label.fill_(????)\n","\n","        errD_real = criterion(output, label)\n","        # Calculate gradients for D in backward pass\n","        errD_real.backward()\n","        D_x = output.mean().item()\n","\n","        ## Train with all-fake batch\n","        # Generate batch of latent vectors\n","        noise = torch.randn(b_size, nz, 1, 1, device=device)\n","        # Generate fake image batch with G\n","        fake = netG(noise)\n","        \n","\n","        # Classify all fake batch with D\n","        output = netD(fake.detach()).view(-1)\n","        # Calculate D's loss on the all-fake batch\n","        \n","        #### TODO \n","        label.fill_(????)\n","        \n","        errD_fake = criterion(output, label)\n","        # Calculate the gradients for this batch\n","        errD_fake.backward()\n","        D_G_z1 = output.mean().item()\n","        # Add the gradients from the all-real and all-fake batches\n","        errD = errD_real + errD_fake\n","        # Update D\n","        optimizerD.step()\n","\n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        netG.zero_grad()\n","        \n","        #### TODO \n","        label.fill_(????)  # fake labels are real for generator cost\n","\n","\n","        # Since we just updated D, perform another forward pass of all-fake batch through D\n","        output = netD(fake).view(-1)\n","        # Calculate G's loss based on this output\n","        errG = criterion(output, label)\n","        # Calculate gradients for G\n","        errG.backward()\n","        D_G_z2 = output.mean().item()\n","        # Update G\n","        optimizerG.step()\n","        \n","        # Output training stats\n","        if i % 50 == 0:\n","            dsp_str = '[%d/%d][%d/%d]' % (epoch, num_epochs, i, len(celeba_dataloader))\n","            print('%s\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n","                  % (dsp_str, errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n","            # Check how the generator is doing by saving G's output on fixed_noise        \n","            with torch.no_grad():\n","                fake = netG(fixed_noise).detach().cpu()\n","            fake = vutils.make_grid(fake[:64], padding=2, normalize=True)\n","            plt.axis(\"off\")\n","            plt.title(\"Generated fake Images\")\n","            plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True),(1,2,0)))\n","            plt.pause(1)\n","\n","\n","        # Save Losses for plotting later\n","        G_losses.append(errG.item())\n","        D_losses.append(errD.item())\n","        \n","            \n","        iters += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oMXacgs7WQU"},"source":["optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n","errD = loss_real + loss_fake\n","errD.backward()\n","optimizerD.step()\n","\n","\n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n","output = netD(fake).view(-1)\n","errG = criterion(output, 1)\n","errG.backward()\n","optimizerG.step()"],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab4.ipynb","provenance":[{"file_id":"13uQuFI591XrelKpUHMcHzWsNibEoZkvS","timestamp":1633504918829},{"file_id":"1hR-DQvve8uEX2zH8h4y1XgP1atKRUl0g","timestamp":1633281385748},{"file_id":"15ec--mRyf2Trd2zwdqWqLM_Qc4JX1iNw","timestamp":1551367509167}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5Ag0r1CeJ-7X"},"source":["# CSCI 3343 Lab 4: Pytorch for Multilayer Perceptron Model\n","\n","**Posted:** Wednesday, October 6, 2021\n","\n","**Due (suggested):** Wednesday, October 13, 2021\n","\n","__Total Points__: 0.5 (extra pts for the final grade)\n","\n","__Name__:\n","[Your first name] [Your last name], [Your BC username]\n","\n","(e.g. Donglai Wei, weidf)\n","\n","__Submission__: please rename the .ipynb file as __\\<your_username\\>_lab4.ipynb__ before you submit it to canvas. Example: weidf_lab4.ipynb.\n","\n","Acknowledgement: Tongzhou Wang (MIT course 6.869)"]},{"cell_type":"markdown","metadata":{"id":"CPvy-9UPKU9l"},"source":["#Introduction\n","\n","We will review Lecture 8 and Lecture 9 on how to build a multilayer perceptron (MLP) model and how to compute the backpropagation manually.\n","\n","## Colab setup: running in GPU session\n","Runtime -> Change runtime type -> Hardware accelerator -> GPU"]},{"cell_type":"markdown","metadata":{"id":"I7kHfyrM6gaC"},"source":["# Part 1. Build a MLP model"]},{"cell_type":"markdown","metadata":{"id":"4kJwrM8O-1Ps"},"source":["## 2-layer MLP (1 hidden layer)\n","To build a deep learning model in Pytorch, we need to define the needed layers under `__init__()` and specify the model computation under `foward()`. The gradient computation is automatically done under the parent's `backward()` (can be overwritten if needed)."]},{"cell_type":"code","metadata":{"id":"sNnhmOmS8VCD"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MLP_oneHiddenLayer(nn.Module):\n","  def __init__(self, input_dim, output_dim, num_neuron, nonlinear=F.relu):\n","    super(MLP_oneHiddenLayer, self).__init__()\n","\n","    self.fc1 = nn.Linear(input_dim, num_neuron)\n","    self.fc2 = nn.Linear(num_neuron, output_dim)\n","    self.nonlinear = nonlinear    \n","   \n","  def forward(self, x):\n","    x = torch.flatten(x, 1) # flatten all dimensions except batch\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)\n","    return F.softmax(x, dim=1)\n","  \n","# print(MLP_oneHiddenLayer(4,3,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZ6IzqhKAYiC"},"source":["! pip install hiddenlayer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7pg8H0_AmDu"},"source":["import hiddenlayer as hl\n","\n","num_input, num_output, num_neuron = 10,20,1 \n","model = MLP_oneHiddenLayer(num_input, num_output, num_neuron)\n","\n","hl.build_graph(model, torch.zeros([1, num_input]), transforms='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4L6KqMOK-sts"},"source":["## Exercise 1. N-layer MLP\n"]},{"cell_type":"markdown","metadata":{"id":"C-ZqgCXsZF0b"},"source":["### (a) **[TODO]** Fill in the missing code.\n","\n","Let's build a MLP model with the specified number of hidden layers and number of neurons.\n","\n","**Course material: Lab 4, page 11**"]},{"cell_type":"code","metadata":{"id":"A1O5rw1N6gkH"},"source":["class MLP(nn.Module):\n","  def __init__(self, input_dim, output_dim, num_neuron=[], nonlinear=F.relu):\n","    super(MLP, self).__init__()\n","\n","    layers = []\n","    if len(num_neuron) == 0:\n","      layers += [nn.Linear(input_dim, output_dim)]\n","    else:\n","      layers += [nn.Linear(input_dim, num_neuron[0]), nn.ReLU()]\n","      for i in range(len(num_neuron)-1):\n","        #### TODO ####\n","        layers += ???\n","      layers += [nn.Linear(num_neuron[-1], output_dim)]    \n","    self.layers = nn.Sequential(*layers)\n","    \n","   \n","  def forward(self, x):\n","    x = torch.flatten(x, 1) # flatten all dimensions except batch    x = x.view(-1, 32*32*3)\n","    x = self.layers(x)\n","    return F.softmax(x, dim=1)\n","\n","# test case \n","num_input, num_output = 10,20\n","num_neuron = [128, 128, 128]\n","model_mlp = MLP(num_input, num_output, num_neuron)\n","\n","hl.build_graph(model_mlp, torch.zeros([1, num_input]), transforms='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HyVwnm9R9iH4"},"source":["# Part 2. Backpropagation by Hand (Exercise 2)\n","\n","Here, let's compute the gradient for each variable by hand and compare with Pytorch's autograd result.\n","\n","To get started, let's create a 2-layer MLP model (1 hidden layer) and make the forward pass."]},{"cell_type":"code","metadata":{"id":"9eLGG_0v9iO-"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# create a MLP model with all intermediate variables\n","class MLP_oneHiddenLayer_var(nn.Module):\n","  def __init__(self, input_dim, output_dim, num_neuron, nonlinear=F.relu):\n","    super(MLP_oneHiddenLayer_var, self).__init__()\n","\n","    self.fc0 = nn.Linear(input_dim, num_neuron)\n","    self.fc1 = nn.Linear(num_neuron, output_dim)\n","    self.nonlinear = nonlinear    \n","    self.x1, self.x2, self.x3, self.x4 = None, None, None, None\n","   \n","  def forward(self, x):\n","    x = torch.flatten(x, 1) # flatten all dimensions except batch\n","    self.x1 = self.fc0(x)\n","    self.x2 = F.relu(self.x1)\n","    self.x3 = self.fc1(self.x2)\n","    self.x4 = F.softmax(self.x3, dim=1)\n","\n","    # by default/to save memory, the gradient for non-leaf nodes\n","    # (intermediate variables in the computation graph) won't be saved\n","    self.x1.retain_grad()\n","    self.x2.retain_grad()\n","    self.x3.retain_grad()\n","    self.x4.retain_grad()\n","\n","    return self.x4\n","  \n","model = MLP_oneHiddenLayer_var(input_dim=10, output_dim=20, num_neuron=5)\n","# input size: batch size x input dimension\n","input = torch.rand([1,10])\n","\n","# forward pass\n","output = model(input)\n","target = torch.rand([1,20])\n","# reduction='sum': L2 norm of the difference\n","loss = F.mse_loss(output, target, reduction = 'sum')\n","\n","# backward pass (autograd)\n","loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vPU2lxdFAIay"},"source":["## (a) **[TODO]** Compute the gradient of the loss layer.\n","\n","**Couse material: Lab 4, page 26**"]},{"cell_type":"code","metadata":{"id":"WLHgnXlZAIjC"},"source":["grad_x4_pt = model.x4.grad\n","\n","#### TODO ####\n","grad_x4_manual = ???\n","\n","print('x4: max difference between gt and yours:', (grad_x4_pt - grad_x4_manual.reshape(-1)).abs().max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQ01SgEmExQv"},"source":["## (b) **[TODO]** Compute the gradient of the nonlinear layer (softmax).\n","\n","In class, the MLP model uses sigmoid for the binary classifaction task. For the softmax function, the input $x_3$ and output $x_4$ both have N dimension. Thus, the local derivative $\\dfrac{\\partial x_4}{\\partial x_3}$ have the dimension NxN (remember, need to compute every pair of variables between $x_4[i]$ and $x_3[j]$), which is the Jacobian matrix.\n","\n","The formula is $\\dfrac{\\partial x_4[i]}{\\partial x_3[j]}=x_4[i](\\delta_{i=j}-x_4[j])$, where $\\delta_{i=j}=1$ when $i==j$ and 0 otherwise.\n","\n","\n","$\\begin{align}\n","\\dfrac{\\partial x_4}{\\partial x_3} &= \\begin{pmatrix}x_4[0](1-x_4[0]) & -x_4[0]x_4[1] & \\dots & -x_4[0]x_4[N-1]\\\\ -x_4[1]x_4[0] & x_4[1](1-x_4[1]) & \\dots & -x_4[1]x_4[N-1]\\\\ \\vdots & \\vdots & \\ddots&\\vdots\\\\ x_4[N-1]x_4[0] & -x_4[N-1]x_4[1] & \\dots & x_4[N-1](1-x_4[N-1]) \\end{pmatrix}\\\\\n","&= \\begin{pmatrix}x_4[0] & 0 & \\dots & 0\\\\ 0 & x_4[1] & \\dots & 0\\\\ \\vdots & \\vdots & \\ddots&\\vdots\\\\ 0 & 0 & \\dots & x_4[N-1]) \\end{pmatrix}- x_4^Tx_4,\n","\\end{align}$\n","\n","where $x_4$ has the dimension 1xN\n","\n","[[Link for more details]](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)\n","\n","\n","**Couse material: Lab 4, page 27**\n","\n","Hints: in Pytorch, `a @ b` means matrix multiplication (which you need!), while `a*b` means element-wise multiplication."]},{"cell_type":"code","metadata":{"id":"WOQx7zXwCaK9"},"source":["grad_x3_pt = model.x3.grad\n","\n","#### TODO ####\n","# jacobian matrix\n","dx4_dx3 = ???\n","\n","grad_x3_manual = ???\n","\n","print('x3: Max difference between gt and yours:', (grad_x3_pt - grad_x3_manual.reshape(-1)).abs().max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-E78PSq1Mr8c"},"source":["## (c) **[TODO]** Compute the gradient of the Linear layer. \n","The $W$ in the slide is the concatenation of $W$ and $b$: `W = [fc.weight, fc.bias.reshape(-1,1)]`\n","\n","\n","**Couse material: Lab 4, page 30**"]},{"cell_type":"code","metadata":{"id":"PfIExT4QMeW0"},"source":["grad_x2_pt = model.x2.grad\n","grad_W1_W_pt = model.fc1.weight.grad\n","grad_W1_b_pt = model.fc1.bias.grad\n","\n","#### TODO ####\n","grad_x2_manual = ???\n","grad_W1_W_manual = ???\n","grad_W1_b_manual =  ???\n","\n","print('x2: max difference between gt and yours:', (grad_x2_pt - grad_x2_manual.reshape(-1)).abs().max())\n","print('W1_W: max difference between gt and yours:', (grad_W1_W_pt.reshape(-1) - grad_W1_W_manual.reshape(-1)).abs().max())\n","print('W1_b: max difference between gt and yours:', (grad_W1_b_pt.reshape(-1) - grad_W1_b_manual.reshape(-1)).abs().max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KEb_sxKIRXWQ"},"source":["## (d) **[TODO]** Compute the gradient of the Nonlinear layer (ReLU).\n","\n","**Couse material: Lab 4, page 27&29**"]},{"cell_type":"code","metadata":{"id":"cgGZrpKwRXeP"},"source":["grad_x1_pt = model.x1.grad\n","\n","#### TODO ####\n","grad_x1_manual = ???\n","\n","print('x1: max difference between gt and yours:', (grad_x1_pt - grad_x1_manual.reshape(-1)).abs().max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ikARzNrR1al"},"source":["## (e) **[TODO]** Compute the gradient of the Linear layer.\n","This the gradient for $W_0$ and $b_0$, so that we can do gradient descent.\n","\n","**Couse material: Lab 4, page 30**"]},{"cell_type":"code","metadata":{"id":"kXN1h_cMHsH-"},"source":["grad_W0_W_pt = model.fc0.weight.grad\n","grad_W0_b_pt = model.fc0.bias.grad\n","\n","#### TODO ####\n","grad_W0_W_manual = ???\n","grad_W0_b_manual =  ???\n","\n","print('W0_W: max difference between gt and yours:', (grad_W0_W_pt.reshape(-1) - grad_W0_W_manual.reshape(-1)).abs().max())\n","print('W0_b: max difference between gt and yours:', (grad_W0_b_pt.reshape(-1) - grad_W0_b_manual.reshape(-1)).abs().max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pRpqPgxRKqKZ"},"source":["# Part 3. Train a MLP model for CIFAR10 classification\n","\n","The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. (Source: [Wikipedia](https://en.wikipedia.org/wiki/CIFAR-10))\n","\n","**Course material: self-study Lab 4, page 39-45**"]},{"cell_type":"code","metadata":{"id":"b8EM0Ixgc7i_"},"source":["!pip install tensorboardcolab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7ZshRua_A-H"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils, datasets\n","\n","# Important: tensorboardcolab only works with tensorflow 1.x\n","%tensorflow_version 1.x\n","from tensorboardcolab import TensorBoardColab"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pMiohf4HLOyJ"},"source":["## 1.1 Data: Download and Dataloader"]},{"cell_type":"markdown","metadata":{"id":"nfYCuMW_L8kM"},"source":["### (a) Download: use Pytorch datasets library"]},{"cell_type":"code","metadata":{"id":"0APSGAPfJ-Ki"},"source":["# transform: will be applied in the iteration of the dataloader\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","cifar10_train_val = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","\n","cifar10_train, cifar10_val = torch.utils.data.random_split(cifar10_train_val, [45000, 5000])\n","cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","print('Class labels:', cifar10_test.classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ZVrfItXo-Sf"},"source":["### (b) Dataloader: sample a batch\n","**Course material: self-study Lab 4, page 41-43**"]},{"cell_type":"code","metadata":{"id":"v0oWcnx_MKnu"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from torchvision.utils import make_grid\n","\n","# functions to show an image\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","batch_size = 16\n","data_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=batch_size,\n","                                         shuffle=True, num_workers=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFm1UcvEqOTP"},"source":["# get some random test images\n","dataiter = iter(data_loader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(make_grid(images, nrow=4))\n","# print labels\n","for i in range(4):\n","  print(' '.join('%5s' % cifar10_test.classes[labels[j]] for j in range(i*4, (i+1)*4)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8S0AeCsWTxz6"},"source":["## 1.2 Model"]},{"cell_type":"markdown","metadata":{"id":"cr-Yc3CCqoW-"},"source":["### (a) Hypothesis Space (Model Architecture)\n","\n","**Course material: self-study Lab 4, page 40**"]},{"cell_type":"code","metadata":{"id":"cYRWlQrYxKMc"},"source":["class MLP_basic(nn.Module):\n","  def __init__(self):\n","    super(MLP_basic, self).__init__()\n","    self.fc1 = nn.Linear(32*32*3, 256)\n","    self.fc2 = nn.Linear(256, 10)\n","   \n","  def forward(self, x):\n","    x = torch.flatten(x, 1) # flatten all dimensions except batch    x = x.view(-1, 32*32*3)\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)\n","    return F.softmax(x, dim=1)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GLGcn7UqqoH"},"source":["### (b) Trainer function\n","For each epoch, the model goes through all the data. It sets up a tensoboard visualization to monitor the training loss in real time."]},{"cell_type":"code","metadata":{"id":"ZnDqxE2msZbd"},"source":["class Config:  \n","  def __init__(self, **kwargs):\n","    # util\n","    self.batch_size = 16\n","    self.epochs = 0\n","    self.save_model_path = '' # use your google drive path to save the model\n","    self.log_interval = 100 # display after number of batches\n","    self.criterion = F.cross_entropy # loss for classification\n","    self.mode = 'train'\n","    for key, value in kwargs.items():\n","      setattr(self, key, value)\n","   \n","class Trainer:  \n","  def __init__(self, model, config, train_data = None, test_data = None):    \n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.epochs = config.epochs\n","    self.save_model_path = config.save_model_path\n","    self.log_interval = config.log_interval\n","    self.mode = config.mode\n","\n","    self.globaliter = 0    \n","    batch_size = config.batch_size\n","    if self.mode == 'train': # training mode\n","      self.train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","                                          shuffle=True, num_workers=1)      \n","      self.tb = TensorBoardColab()\n","      self.optimizer = config.optimizer\n","    \n","    if test_data is not None: # need evaluation\n","      self.test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n","                                         shuffle=False, num_workers=1)\n","    \n","    self.model = model.to(self.device)\n","    self.criterion = config.criterion # loss function\n","    \n","                \n","  def train(self, epoch):  \n","    self.model.train()\n","    for batch_idx, (data, target) in enumerate(self.train_loader):\n","      \n","      self.globaliter += 1\n","      data, target = data.to(self.device), target.to(self.device)\n","\n","      self.optimizer.zero_grad()\n","      predictions = self.model(data)\n","\n","      loss = self.criterion(predictions, target)\n","      loss.backward()\n","      self.optimizer.step()\n","\n","      if batch_idx % self.log_interval == 0:\n","        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                  epoch, batch_idx * len(data), len(self.train_loader.dataset),\n","                  100. * batch_idx / len(self.train_loader), loss.item()))\n","        self.tb.save_value('Train Loss', 'train_loss', self.globaliter, loss.item())\n","        self.tb.flush_line('train_loss')\n","        \n","        \n","  def test(self, epoch):\n","    self.model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","      print('Start testing...')\n","      for data, target in self.test_loader:\n","        data, target = data.to(self.device), target.to(self.device)\n","        predictions = self.model(data)\n","        test_loss += self.criterion(predictions, target, reduction='sum').item()\n","        prediction = predictions.argmax(dim=1, keepdim=True)\n","        correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","      test_loss /= len(self.test_loader.dataset)\n","      accuracy = 100. * correct / len(self.test_loader.dataset)\n","\n","      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","          test_loss, correct, len(self.test_loader.dataset), accuracy))\n","      if self.mode == 'train': # add validation data to tensorboard\n","        self.tb.save_value('Validation Loss', 'val_loss', self.globaliter, test_loss)\n","        self.tb.flush_line('val_loss')\n","\n","  def main(self):    \n","    if self.mode == 'train':\n","      for epoch in range(1, self.epochs + 1):          \n","          self.train(epoch)\n","          if self.test_loader is not None:\n","            # exist validation data\n","            self.test(epoch)\n","    elif self.mode == 'test':\n","      self.test(0)\n","          \n","    if (self.save_model_path != ''):\n","        torch.save(self.model.state_dict(), self.save_model_path + \"/cifar10_cnn.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8D_qDz9brfyq"},"source":["### (c) Check out the initial model accuracy. \n","The performance is similar to that of the random guess (10%)."]},{"cell_type":"code","metadata":{"id":"jvxJvdZ7POeA"},"source":["# create a model\n","model = MLP_basic()\n","\n","# Obtain its performance on the test data\n","test_config = Config(mode='test')\n","Trainer(model, test_config, test_data = cifar10_test).main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UDBO1_Fmru9I"},"source":["### (d) Train the model\n","We will use the cross entropy loss for the multiclass prediction: `F.cross_entropy`. Let's train the model!\n","\n","\n","Hint: \n","- Click on the tensorboard link and need to refresh for latest training losses\n","- Free feel to tweak the hyperparameters to improve the result.\n","\n","**Course material: self-study Lab 4, page 40, 44-45**"]},{"cell_type":"code","metadata":{"id":"XL9j-q56ouUB"},"source":["# model training\n","\n","# set of hyperparameters\n","train_config = Config(    \n","    # dataloader\n","    batch_size = 16,\n","    # loss\n","    criterion = F.cross_entropy,\n","    # optimization\n","    optimizer = optim.SGD(model.parameters(), lr=0.01),    \n","    epochs = 10,\n","    # util\n","    save_model_path = '', # if you like, use your google drive path to save the model (mount google drive first)\n","    log_interval = 100 # display after number of batches\n",")\n","\n","Trainer(model, train_config, cifar10_train, cifar10_val).main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPTzS5cLsuh8"},"source":["### (e) Test the model\n","After you find the best model on the validation data, test the model on the real test data."]},{"cell_type":"code","metadata":{"id":"MUbXqgRMf1aY"},"source":["# test results\n","Trainer(model, test_config, test_data = cifar10_test).main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HA4Px_HMuZ2e"},"source":["## Exercise 3. Train your MLP models\n"]},{"cell_type":"markdown","metadata":{"id":"6mrQ0O5zxrsa"},"source":["### (a) **[TODO]** Visualize the fc layer weights for the linear classification.\n","For the linear classification, e.g., logistic regression, the learned weights are templates for each class. Intuitively, the model classifies the image by doing dot product to see which template the input image is close to. Let's train a model and verify this."]},{"cell_type":"code","metadata":{"id":"QIxOJLlsxr6K"},"source":["class LogisticRegression(nn.Module):\n","  def __init__(self):\n","    super(LogisticRegression, self).__init__()\n","    #### TODO ####\n","    ??? \n","   \n","  def forward(self, x):\n","    x = torch.flatten(x, 1) # flatten all dimensions except batch    x = x.view(-1, 32*32*3)    \n","    #### TODO ####\n","    ???\n","    return F.softmax(x, dim=1)\n","\n","model_lr = LogisticRegression()\n","# model training\n","\n","# set of hyperparameters\n","train_config = Config(    \n","    # dataloader\n","    batch_size = 16,\n","    # loss\n","    criterion = F.cross_entropy,\n","    # optimization\n","    optimizer = optim.SGD(model_lr.parameters(), lr=0.01),    \n","    epochs = 5,\n","    # util\n","    save_model_path = '', # if you like, use your google drive path to save the model (mount google drive first)\n","    log_interval = 100 # display after number of batches\n",")\n","\n","Trainer(model_lr, train_config, cifar10_train, cifar10_val).main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXD5kFx5x7eJ"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# visualization code\n","# get the weight\n","W =  model_lr.fc1.weight.detach().cpu().numpy()\n","# scale the range to 0-1\n","W = (W-W.min()) / (W.max()-W.min())\n","\n","for i in range(10):\n","  plt.subplot(3, 4, i+1)\n","  npimg = W[i].reshape(3, 32, 32)\n","  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","  plt.axis('off')\n","  plt.title(cifar10_test.classes[i])  \n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cYjGbOe8YbZ4"},"source":["### (b) **[TODO]** Add BatchNorm\n","The convergence of the model is above is slow. It's hard to get a model with more than 50% accuracy. Here, let's add the magic normalization layers to speed things up.\n","\n","**Course material: Lecture 10**\n","\n"]},{"cell_type":"code","metadata":{"id":"tMKpdICquaAD"},"source":["class MLP_bn(nn.Module):\n","  def __init__(self):\n","    super(MLP_bn, self).__init__()\n","    self.fc1 = nn.Linear(32*32*3, 256)\n","    self.bn1 = torch.nn.BatchNorm1d(256)\n","    self.fc2 = nn.Linear(256, 10)\n","   \n","  def forward(self, x):\n","    x = torch.flatten(x, 1) # flatten all dimensions except batch    x = x.view(-1, 32*32*3)\n","    #### TODO ####\n","    # -> linear -> bn -> relu\n","    x = ???\n","    x = self.fc2(x)\n","    return F.softmax(x, dim=1)\n","    \n","model_bn = MLP_bn()\n","# set of hyperparameters\n","train_config = Config(    \n","    # dataloader\n","    batch_size = 16,\n","    # loss\n","    criterion = F.cross_entropy,\n","    # optimization\n","    optimizer = optim.SGD(model_bn.parameters(), lr=0.01),    \n","    epochs = 10,\n","    # util\n","    save_model_path = '', # use your google drive path to save the model\n","    log_interval = 100 # display after number of batches\n",")\n","\n","Trainer(model_bn, train_config, cifar10_train, cifar10_val).main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJj-zxDjvwFl"},"source":["Trainer(model_bn, test_config, test_data = cifar10_test).main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8GmsO-IFZSGd"},"source":["### (c) **[TODO]** Train a deeper MLP with BatchNorm layers\n","With the BatchNorm layer, we can train deeper MLP models more effectively. Please define your own model and get the test result better than 50%.\n","\n","Hints: Exercise 1\n"]},{"cell_type":"code","metadata":{"id":"dJNDLTQsZh39"},"source":["class MLP_bn2(nn.Module):\n","  def __init__(self, input_dim=32*32*3, output_dim=10, num_neuron=[256]):\n","    super(MLP_bn2, self).__init__()\n","    layers = []\n","    #### TODO ####\n","    ???\n","    self.layers = nn.Sequential(*layers)\n","   \n","  def forward(self, x):\n","    x = torch.flatten(x, 1) # flatten all dimensions except batch    x = x.view(-1, 32*32*3)\n","    x = self.layers(x)    \n","    return F.softmax(x, dim=1)\n","    \n","model_bn2 = MLP_bn2()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CoMKaDScb_Qy"},"source":["# check your defined model\n","print(model_bn2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsdqPtLtcib-"},"source":["# train the model with the following hyperparameters\n","train_config = Config(    \n","    # dataloader\n","    batch_size = 16,\n","    # loss\n","    criterion = F.cross_entropy,\n","    # optimization\n","    optimizer = optim.Adam(model_bn2.parameters(), lr=0.01),    \n","    epochs = 10,\n","    # util\n","    save_model_path = '', # use your google drive path to save the model\n","    log_interval = 100 # display after number of batches\n",")\n","\n","Trainer(model_bn2, train_config, cifar10_train, cifar10_val).main()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h12oNknwgali"},"source":["# evaluate the model on the test data\n","Trainer(model_bn2, test_config, test_data = cifar10_test).main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PN2kd6e4gkG5"},"source":["# hint: you can continue to train your model with a smaller learning rate if the current model seems to get stuck"],"execution_count":null,"outputs":[]}]}
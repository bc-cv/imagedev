{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab5.ipynb","provenance":[{"file_id":"1hR-DQvve8uEX2zH8h4y1XgP1atKRUl0g","timestamp":1633281385748},{"file_id":"15ec--mRyf2Trd2zwdqWqLM_Qc4JX1iNw","timestamp":1551367509167}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5Ag0r1CeJ-7X"},"source":["# CSCI 3343 Lab 5: Pytorch for Image Prediction and Object Segmentation\n","\n","**Posted:** Monday, October 18, 2021\n","\n","**Due:** N/A\n","\n","__Total Points__: 0.5 (extra pts for the final grade)\n","\n","__Name__:\n","[Your first name] [Your last name], [Your BC username]\n","\n","(e.g. Donglai Wei, weidf)\n","\n","__Submission__: please rename the .ipynb file as __\\<your_username\\>_lab5.ipynb__ before you submit it to canvas. Example: weidf_lab5.ipynb."]},{"cell_type":"markdown","metadata":{"id":"CPvy-9UPKU9l"},"source":["#Introduction\n","\n","Let's learn to use existing deep learning libraries for image prediction and object detection in PyTorch."]},{"cell_type":"markdown","metadata":{"id":"l3eVA1p3PLPI"},"source":["## Download and display the image"]},{"cell_type":"code","metadata":{"id":"IKO-UCCkOPGG","executionInfo":{"status":"ok","timestamp":1634566064758,"user_tz":240,"elapsed":490,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"d1c2a9e1-f83f-4ddf-94a9-e1af2afcd2b6","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Download images\n","! wget https://post.healthline.com/wp-content/uploads/2020/08/3180-Pug_green_grass-732x549-thumbnail-732x549.jpg -O test_dog_easy.jpg\n","! wget https://www.chicagotribune.com/resizer/Z_oN8fZUymKMakZ7Y-KBqCwwEi0=/800x515/top/arc-anglerfish-arc2-prod-tronc.s3.amazonaws.com/public/BPLQ2KEPMJABHPUP7U565WVMNA.jpg -O test_dog_hard.jpg\n","\n","# Download ImageNet labels\n","!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt -O imagenet_classes.txt\n","with open(\"imagenet_classes.txt\", \"r\") as f:\n","    categories = [s.strip() for s in f.readlines()]"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-18 14:07:44--  https://post.healthline.com/wp-content/uploads/2020/08/3180-Pug_green_grass-732x549-thumbnail-732x549.jpg\n","Resolving post.healthline.com (post.healthline.com)... 151.101.2.133, 151.101.66.133, 151.101.130.133, ...\n","Connecting to post.healthline.com (post.healthline.com)|151.101.2.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 45602 (45K) [image/jpeg]\n","Saving to: ‚Äòtest_dog_easy.jpg‚Äô\n","\n","\rtest_dog_easy.jpg     0%[                    ]       0  --.-KB/s               \rtest_dog_easy.jpg   100%[===================>]  44.53K  --.-KB/s    in 0.001s  \n","\n","2021-10-18 14:07:44 (30.8 MB/s) - ‚Äòtest_dog_easy.jpg‚Äô saved [45602/45602]\n","\n","--2021-10-18 14:07:44--  https://www.chicagotribune.com/resizer/Z_oN8fZUymKMakZ7Y-KBqCwwEi0=/800x515/top/arc-anglerfish-arc2-prod-tronc.s3.amazonaws.com/public/BPLQ2KEPMJABHPUP7U565WVMNA.jpg\n","Resolving www.chicagotribune.com (www.chicagotribune.com)... 23.12.147.85, 23.12.147.69, 2600:1408:c400:e::17cd:6a0d, ...\n","Connecting to www.chicagotribune.com (www.chicagotribune.com)|23.12.147.85|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 63717 (62K) [image/jpeg]\n","Saving to: ‚Äòtest_dog_hard.jpg‚Äô\n","\n","test_dog_hard.jpg   100%[===================>]  62.22K  --.-KB/s    in 0.002s  \n","\n","2021-10-18 14:07:44 (29.2 MB/s) - ‚Äòtest_dog_hard.jpg‚Äô saved [63717/63717]\n","\n","--2021-10-18 14:07:44--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10472 (10K) [text/plain]\n","Saving to: ‚Äòimagenet_classes.txt‚Äô\n","\n","imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n","\n","2021-10-18 14:07:44 (74.6 MB/s) - ‚Äòimagenet_classes.txt‚Äô saved [10472/10472]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"hJ4_ORp6OVKj"},"source":["import imageio\n","import matplotlib.pyplot as plt\n","\n","dog_easy = imageio.imread('test_dog_easy.jpg')\n","dog_hard = imageio.imread('test_dog_hard.jpg')\n","\n","plt.rcParams[\"figure.figsize\"] = (10,5)\n","plt.subplot(121)\n","plt.imshow(dog_easy)\n","plt.axis('off')\n","plt.title('easy case')\n","plt.subplot(122)\n","plt.imshow(dog_hard)\n","plt.axis('off')\n","plt.title('hard case')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7kHfyrM6gaC"},"source":["# Part 1. Image Classification with PyTorch"]},{"cell_type":"markdown","metadata":{"id":"Qvmz3p9pSu-I"},"source":["## (a) Download the AlexNet model and 1,000 class labels"]},{"cell_type":"code","metadata":{"id":"aCgKFqO-SvHx"},"source":["import torch\n","import torch.nn.functional as F\n","import torchvision.models as models\n","\n","from torchvision import transforms\n","import numpy as np\n","from PIL import Image\n","\n","# Download models\n","alexnet = models.alexnet(pretrained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykgCpCPFPRik"},"source":["## (b) Preprocess image"]},{"cell_type":"code","metadata":{"id":"fmr73wAUQhDt"},"source":["from PIL import Image\n","\n","preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    # if only takes the center crop\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    # if takes 10 crops: (1 center + 4 corners) * (original + horizontal flip)\n","    #transforms.TenCrop(224),\n","    #transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),    \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# PyTorch only takes image in PIL format\n","\n","im_batch = torch.stack([preprocess(Image.fromarray(dog_easy)), \\\n","                        preprocess(Image.fromarray(dog_hard))])\n","if im_batch.ndim == 3:\n","  # [None]: create an extra dimension in front as the batch\n","  im_batch = im_batch[None]\n","\n","print('input batch size:', im_batch.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZJSR7zBYGcJ"},"source":[" ## (c) Run inference"]},{"cell_type":"code","metadata":{"id":"WYZnIo8hOTJ1"},"source":["# the model doesn't include the softmax layer\n","pred = alexnet(im_batch)\n","prob = F.softmax(pred, dim=1)\n","\n","for i in range(pred.shape[0]):\n","  print('------ %s -----' % (['Easy case', 'Hard case'][i]))\n","  # Show top categories per image\n","  top5_prob, top5_catid = torch.topk(prob[i], 5)\n","  for i in range(top5_prob.size(0)):\n","      print(categories[top5_catid[i]], '%.2f'%top5_prob[i].item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dal9EcQMYkXS"},"source":["## [TODO] Exercise 1: Image classification with ResNet50 for these two images\n","Hint: repeat (a) and (c) above with the new model"]},{"cell_type":"code","metadata":{"id":"tm0QV_DCYjm2"},"source":["#### TODO #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HyVwnm9R9iH4"},"source":["# Part 2. Object Detection with PyTorch\n","\n","Let's try out different pipelines for the hard case above to detect the dog."]},{"cell_type":"code","metadata":{"id":"-3gwvt-BocmE"},"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import numpy as np\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","# Download models\n","alexnet = torchvision.models.alexnet(pretrained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJMh85oLauMX"},"source":["## (a) Sliding CNN\n","Let's iterative through the bounding boxes with a certain size (square patch: 151x151) with a fixed stride (71x71). In practice, we need to repeat the computation above for different sizes of bounding boxes."]},{"cell_type":"markdown","metadata":{"id":"s2pzsuX1vb1A"},"source":["### (i) Get bounding boxes (sliding window)"]},{"cell_type":"code","metadata":{"id":"t8P-YZw3bTIl"},"source":["def imageToPatch(img, row_id, col_id, patch_size, stride_size):\n","  return img[stride_size*row_id : stride_size*row_id+patch_size,\\\n","             stride_size*col_id : stride_size*col_id+patch_size]\n","\n","# image to patches\n","dog_hard = imageio.imread('test_dog_hard.jpg')\n","im_size = dog_hard.shape\n","patch_size = 151\n","stride_size = 71\n","num_row = (im_size[0] - patch_size) // stride_size + 1\n","num_col = (im_size[1] - patch_size) // stride_size + 1\n","print('#row=%d, #col=%d' % (num_row, num_col))\n","\n","plt.rcParams[\"figure.figsize\"] = (10,6)\n","count = 1\n","for y in range(num_row):\n","  for x in range(num_col):\n","    plt.subplot(num_row, num_col, count)\n","    patch = imageToPatch(dog_hard, y, x, patch_size, stride_size)\n","    plt.imshow(patch)\n","    plt.axis('off')\n","    count += 1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WC_V4G0KvjE_"},"source":["### (ii) Preprocess image patches"]},{"cell_type":"code","metadata":{"id":"9eLGG_0v9iO-"},"source":["preprocess = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# convert image into patches\n","patches = []\n","for y in range(num_row):\n","  for x in range(num_col):\n","    patch = imageToPatch(dog_hard, y, x, patch_size, stride_size)\n","    patches.append(preprocess(Image.fromarray(patch)))\n","\n","im_batch = torch.stack(patches)\n","print('input batch size:', im_batch.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GyOTqTpkvxm4"},"source":["### (iii) Run inference"]},{"cell_type":"code","metadata":{"id":"CgLww1S9bMPT"},"source":["pred = alexnet(im_batch)\n","prob = F.softmax(pred, dim=1).detach().numpy()\n","\n","# plot the top probability for each patch\n","plt.rcParams[\"figure.figsize\"] = (10,2)\n","prob_max = prob.max(axis=1)\n","plt.plot(prob_max)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NkkqAn91duDB"},"source":["# let's see what does it detect for the most confident patch\n","pos_ids = np.where(prob_max > 0.3)[0]\n","row_ids = pos_ids // num_col\n","col_ids = pos_ids - row_ids * num_col\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","for i in range(len(pos_ids)):\n","  plt.subplot(3,3,i+1)\n","  patch = imageToPatch(dog_hard, row_ids[i], col_ids[i], patch_size, stride_size)\n","  plt.imshow(patch)\n","  plt.axis('off')\n","  plt.title(categories[np.argmax(prob[pos_ids[i]])])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vPU2lxdFAIay"},"source":["## (b) R-CNN\n"]},{"cell_type":"markdown","metadata":{"id":"7rwU_9uFvY0a"},"source":["### (i) Get bounding boxes (selective search)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLHgnXlZAIjC","executionInfo":{"status":"ok","timestamp":1634539042880,"user_tz":240,"elapsed":7175,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"bd0f78f1-2630-4e7a-8c98-7738cfaf7137"},"source":["# cv2 reads in images as BGR; pytorch pretrain models take RGB input\n","# create a new variable to avoid confusion...\n","dog_hard_cv2 = cv2.imread('test_dog_hard.jpg')\n","ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","ss.setBaseImage(dog_hard_cv2)\n","ss.switchToSelectiveSearchFast()\n","# rects: Nx4 matrix\n","# each row: x,y,w,h\n","rects = ss.process()\n","print('Selective search: find %d boxes' % rects.shape[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selective search: find 4782 boxes\n"]}]},{"cell_type":"code","metadata":{"id":"9quSU8JjhMP2"},"source":["# plot the top 20 bounding boxes\n","image = cv2.rectangle(dog_hard_cv2.copy(), tuple(rects[0,:2]), tuple(rects[0,:2]+rects[0,2:]), (0,255,0), 2)\n","for i in range(1, 20):\n","  image = cv2.rectangle(image, tuple(rects[i,:2]), tuple(rects[i,:2]+rects[i,2:]), (0,255,0), 2)\n","cv2_imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x5KjuWLij7IH"},"source":["### (ii) Preprocess image patches"]},{"cell_type":"code","metadata":{"id":"uaZFtxAGh_4M"},"source":["preprocess = transforms.Compose([\n","    transforms.Resize((224,224)), # scale the both sides to 224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# take every 8 bounding boxes\n","rects_sub = rects[::8]\n","patches = [None] * rects_sub.shape[0]\n","for i in range(len(patches)):  \n","  x,y,w,h = rects_sub[i]  \n","  patches[i] = preprocess(Image.fromarray(dog_hard[y:y+h, x:x+w]))\n","\n","im_batch = torch.stack(patches)\n","print('input batch size:', im_batch.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39o3d6wEvWPK"},"source":["### (iii) Run inference"]},{"cell_type":"code","metadata":{"id":"2VKQIz9ClnMr"},"source":["pred = alexnet(im_batch)\n","prob = F.softmax(pred, dim=1).detach().numpy()\n","\n","# plot the top probability for each patch\n","prob_max = prob.max(axis=1)\n","plt.rcParams[\"figure.figsize\"] = (10,2)\n","plt.plot(prob_max)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZ8b5fNxks_9"},"source":["# let's see what does it detect for the most confident patch \n","pos_sort = np.argsort(-prob_max)\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","for i in range(16):\n","  plt.subplot(4,4,i+1)\n","  x,y,w,h = rects_sub[pos_sort[i]]\n","  patch = dog_hard[y:y+h, x:x+w]\n","  plt.imshow(cv2.resize(patch,(224,224)))\n","  plt.axis('off')\n","  plt.title(categories[np.argmax(prob[pos_sort[i]])])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRKk5QpaqepN"},"source":["# Non-maximum suppression\n","rects_sub_pt = torch.as_tensor(np.hstack([rects_sub[:,:2], rects_sub[:,:2]+rects_sub[:,2:]]).astype(np.float32))\n","idx = torchvision.ops.nms(rects_sub_pt, torch.as_tensor(prob_max), 0.1)\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","for i in range(16):\n","  plt.subplot(4,4,i+1)\n","  x,y,w,h = rects_sub[idx[i]]\n","  patch = dog_hard[y:y+h, x:x+w]\n","  plt.imshow(cv2.resize(patch,(224,224)))\n","  plt.axis('off')\n","  plt.title(categories[np.argmax(prob[idx[i]])])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pGtKW3oHxRA-"},"source":["## (c) Fast/Faster R-CNN (Detectron2!)"]},{"cell_type":"markdown","metadata":{"id":"oFWLXXtZy6J3"},"source":["### (i) Install Detectron2"]},{"cell_type":"code","metadata":{"id":"zlui9fE9sQfO"},"source":["!pip install pyyaml==5.1\n","# This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n","# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# Install detectron2 that matches the above pytorch version\n","# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.9/index.html\n","# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sML_VsOk3I7G"},"source":["### (ii) Run inference"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FfQCr66qIeO","executionInfo":{"status":"ok","timestamp":1634533601017,"user_tz":240,"elapsed":434,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"cb89ac5b-dc23-4236-c3d0-0ff552c3e299"},"source":["# check pytorch installation: \n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","assert torch.__version__.startswith(\"1.9\")   # please manually install torch 1.9 if Colab changes its default version\n","\n","# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.9.0+cu111 True\n"]}]},{"cell_type":"code","metadata":{"id":"izX0S6TIy4rH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634536797860,"user_tz":240,"elapsed":1232,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"9c65d99b-ff2d-4304-920a-00fb744b5f7c"},"source":["cfg = get_cfg()\n","# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n","cfg.INPUT.FORMAT = 'RGB'\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_C4_3x.yaml\"))\n","#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n","# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_C4_3x.yaml\")\n","predictor = DefaultPredictor(cfg)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The checkpoint state_dict contains keys that are not used by the model:\n","  \u001b[35mproposal_generator.anchor_generator.cell_anchors.0\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"kPcLLtaZ0Dvm"},"source":["dog_hard = imageio.imread('test_dog_hard.jpg')\n","outputs = predictor(dog_hard)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94JyU9-y1eCX","executionInfo":{"status":"ok","timestamp":1634534585990,"user_tz":240,"elapsed":176,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"f7923e07-6b3f-4382-a059-56fb808e633d"},"source":["outputs['instances'].__dict__['_fields'].keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['pred_boxes', 'scores', 'pred_classes'])"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","metadata":{"id":"UpbnNvP110aX"},"source":["# We can use `Visualizer` to draw the predictions on the image.\n","v = Visualizer(dog_hard, MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n","out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","\n","plt.rcParams[\"figure.figsize\"] = (30,30)\n","plt.imshow(out.get_image())\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EAy2ZBT3SSn"},"source":["###(iii) Model examination"]},{"cell_type":"markdown","metadata":{"id":"87Jw9U7738yy"},"source":["#### Q1. how many modules in the model"]},{"cell_type":"code","metadata":{"id":"woA81YLS1-Jn"},"source":["# what are the modules\n","for module in predictor.model._modules:\n","  print(module)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpSGDYzZ4EK0"},"source":["#### Q2. what's the input and output size of the **backbone** module"]},{"cell_type":"code","metadata":{"id":"LUs73pbF8nqc"},"source":["print('Input image size:', dog_hard.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCNU-DUQ8m11"},"source":["predictor.model.backbone"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnP5getf4arV"},"source":["activation = {}\n","def get_activation(name):\n","    def hook(model, input, output):\n","        activation[name] = output.detach()\n","    return hook\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gb_0Rno56yZW"},"source":["predictor.model.backbone.res4._modules['5'].conv3.norm.register_forward_hook(get_activation('backbone'))\n","outputs = predictor(dog_hard)\n","print('Output feature size:', activation['backbone'].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PKxII1hJ7rKV"},"source":["#### Q3. what's the input and output size of the **proposal_generator** module\n","Check out the size for `objectness_logits` and `anchor_deltas`"]},{"cell_type":"code","metadata":{"id":"_cvGUPnu7wEd"},"source":["predictor.model.proposal_generator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzJeXZoO7tTe"},"source":["predictor.model.proposal_generator.rpn_head.objectness_logits.register_forward_hook(get_activation('objectness_logits'))\n","predictor.model.proposal_generator.rpn_head.anchor_deltas.register_forward_hook(get_activation('anchor_deltas'))\n","\n","outputs = predictor(dog_hard)\n","\n","#print('Output feature size:', (dog_hard_cv2).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecdYg2vO8StN"},"source":["print('Output objectness size:', activation['objectness_logits'].shape)\n","print('Output anchor_deltas size:', activation['anchor_deltas'].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tm6HUeoAwJS"},"source":["#### [TDOO] Exercise 2. How many boxes does the model predict?"]},{"cell_type":"code","metadata":{"id":"Vr4smbl8-wXz"},"source":["????"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXSN_oYd9OU1"},"source":["#### Q4. what's the input and output size of the **roi_heads** module"]},{"cell_type":"code","metadata":{"id":"oNVN_4fC8X_P"},"source":["predictor.model.roi_heads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTbQCUfq9ls0"},"source":["predictor.model.roi_heads.pooler.level_poolers._modules['0'].register_forward_hook(get_activation('level_poolers'))\n","predictor.model.roi_heads.box_predictor.cls_score.register_forward_hook(get_activation('cls_score'))\n","predictor.model.roi_heads.box_predictor.bbox_pred.register_forward_hook(get_activation('bbox_pred'))\n","outputs = predictor(dog_hard)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a65zDRAQ9ylZ","executionInfo":{"status":"ok","timestamp":1634536965036,"user_tz":240,"elapsed":137,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"cb542996-1e4f-4df7-9c6c-90468a485118"},"source":["print('Input level_poolers size:', activation['level_poolers'].shape)\n","print('Output cls_score size:', activation['cls_score'].shape)\n","print('Output bbox_pred size:', activation['bbox_pred'].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input level_poolers size: torch.Size([1000, 1024, 14, 14])\n","Output cls_score size: torch.Size([1000, 81])\n","Output bbox_pred size: torch.Size([1000, 320])\n"]}]},{"cell_type":"code","metadata":{"id":"F6AdTB0jBRfw"},"source":["predictor.model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhLE2aBjEPP0"},"source":["## (d) YOLO"]},{"cell_type":"markdown","metadata":{"id":"6UaHCaMgEYCa"},"source":["### (i) Install YOLO"]},{"cell_type":"code","metadata":{"id":"Yu1JhiXjEYLj"},"source":["! pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkJZn3jVEpVv"},"source":["### (ii) Run inference"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2MB4toHEper","executionInfo":{"status":"ok","timestamp":1634539349845,"user_tz":240,"elapsed":4129,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"504b52a5-ed26-47b8-a163-9b4d8cfbb81a"},"source":["import torch\n","\n","# Model\n","model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n","\n","# Images\n","imgs = ['https://www.chicagotribune.com/resizer/Z_oN8fZUymKMakZ7Y-KBqCwwEi0=/800x515/top/arc-anglerfish-arc2-prod-tronc.s3.amazonaws.com/public/BPLQ2KEPMJABHPUP7U565WVMNA.jpg']  # batch of images\n","\n","# Inference\n","results = model_yolo(imgs)\n","results_np = results.xyxy[0].detach().cpu().numpy().astype(int)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n"]},{"output_type":"stream","name":"stderr","text":["YOLOv5 üöÄ 2021-10-18 torch 1.9.0+cu111 CUDA:0 (Tesla K80, 11441.1875MB)\n","\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (6.0)\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n"]},{"output_type":"stream","name":"stderr","text":["Fusing layers... \n","Model Summary: 213 layers, 7225885 parameters, 0 gradients\n","Adding AutoShape... \n"]}]},{"cell_type":"code","metadata":{"id":"0EGQkpDOHLLu"},"source":["# plot the top 6 bounding boxes\n","image = cv2.rectangle(dog_hard_cv2.copy(), tuple(results_np[0,:2]), tuple(results_np[0,2:4]), (0,255,0), 2)\n","for i in range(1, results_np.shape[0]):\n","  image = cv2.rectangle(image, tuple(results_np[i,:2]), tuple(results_np[i,2:4]), (0,255,0), 2)\n","cv2_imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Axc4M9VWE42m"},"source":["plt.rcParams[\"figure.figsize\"] = (10,10)\n","for i in range(8):\n","  plt.subplot(4,2,i+1)\n","  x1,y1,x2,y2,sc, cls = results_np[i]\n","  patch = dog_hard[y1:y2, x1:x2]\n","  plt.imshow(cv2.resize(patch,(224,224)))\n","  plt.axis('off')\n","  plt.title(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes[int(cls)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1ofk2ZzIBt6"},"source":["### (iii) Model examination"]},{"cell_type":"code","metadata":{"id":"T2I7KlKQIB2L"},"source":["model_yolo "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxPz0foSIdq-"},"source":["model_yolo.model.model._modules['24'].m._modules['2'].register_forward_hook(get_activation('detection'))\n","results = model_yolo(imgs)\n","print('Detection shape', activation['detection'].shape)"],"execution_count":null,"outputs":[]}]}
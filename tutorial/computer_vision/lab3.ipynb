{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"lab3.ipynb","provenance":[{"file_id":"18yIM4-G2U6qPsb-kK5NGko7D3D8fZ8xC","timestamp":1632715225585},{"file_id":"1IzacTpfzNV-sF4O2NjOXU-GumIOWyqZ-","timestamp":1615573487743},{"file_id":"1Sw66kxhwaSEVTkM74hvpK2oxZikseOpc","timestamp":1571367245558}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IAae8nayHhH2"},"source":["# CSCI 3343 Lab 3: Pytorch for Linear Regression and Classification\n","\n","**Posted:** Monday, September 27, 2021\n","\n","**Due:** Monday, October 4, 2021 (11:59 am)\n","\n","__Total Points__: 0.5 (extra pts for the final grade)\n","\n","__Name__:\n","[Your first name] [Your last name], [Your BC username]\n","\n","(e.g. Donglai Wei, weidf)\n","\n","__Submission__: please rename the .ipynb file as __\\<your_username\\>_lab3.ipynb__ before you submit it to canvas. Example: weidf_lab3.ipynb.\n","\n","Acknowledgement: Tongzhou Wang (MIT course 6.869)"]},{"cell_type":"markdown","metadata":{"id":"VZUcz-vU-vKw"},"source":["#Introduction\n","\n","\n","## Colab setup: running in GPU session\n","Runtime -> Change runtime type -> Hardware accelerator -> GPU"]},{"cell_type":"markdown","metadata":{"id":"EKSHYh1gby1h"},"source":["# Part 1. PyTorch Basics\n","\n","We will explain how to implement machine learning building blocks (model, loss, and optimization) in PyTorch. Then, you will be able to redo the \"college entrance classification\" problem ([Lab 2, Part 3](https://colab.research.google.com/drive/1sobb8ZRUbgxzUyJDcoMh7vlHckRrj5qe)) with just a few lines of code!"]},{"cell_type":"markdown","metadata":{"id":"rKJkpn84iMBw"},"source":["## 1.0 Overview"]},{"cell_type":"markdown","metadata":{"id":"2drtxcMec0YU"},"source":["### What is PyTorch?\n","1. A Python GPU-accelerated tensor library (NumPy, but faster)\n","2. Differentiable Programming with dynamic computation graphs\n","3. Flexible and efficient **neural network** library\n","4. Python-first framework (easy to integrate with other Python libraries, debug, and extend)\n","  + Quick conversion from & to NumPy array, integration with other Python libs.\n","  + Your favorite Python debugger.\n","  + Adding custom ops with Python/c++ extension. \n","  + Running in purely c++ environment with the c++ API.\n","\n","Useful links:\n","\n","+ PyTorch documentation: https://pytorch.org/docs/stable/index.html\n","  -  Most math operations can be found as `torch.*` or `Tensor.*`.\n","+ [Optional] PyTorch official tutorials: https://pytorch.org/tutorials/\n","  - Transfer learning tutorial: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n","+ [Optional] PyTorch examples: https://github.com/pytorch/examples/\n","  - DCGAN, ImageNet training, Reinforcement Learning, etc."]},{"cell_type":"markdown","metadata":{"id":"nCij29tonH84"},"source":["### PyTorch Installation"]},{"cell_type":"code","metadata":{"id":"FgmnwpcHf8U0"},"source":["# install torch and torchvision (a utility library for computer vision that provides many public datasets and pre-trained models)\n","!pip install torch torchvision"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mj3oX5WEkM8P"},"source":["## 1.1 Data: GPU-accelerated Tensor Library\n","\n","The syntax of the torch tensor library is similar to that of numpy."]},{"cell_type":"code","metadata":{"id":"dPdEDFjyf94_"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cVfJH3PgvCE"},"source":["# Create a 3x5 matrix filled with zeros\n","x = torch.zeros(3, 5)\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qauqxYzmgQBO"},"source":["# Create a 3x5 matrix filled with random values from a standard normal distribution\n","y = torch.randn(3, 5)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUS9LmmF58gI"},"source":["# Shape manipulations\n","print('\\n.t()  (transpose): ')\n","print(y.t())\n","\n","print('.reshape(5, 3): ')\n","print(y.reshape(5, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-qEF8if6VkQ"},"source":["# Slicing\n","print(y[1:])\n","\n","# Slicing + select every two elements\n","print(y[1:, ::2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGpFKCBEgbJe"},"source":["# Basic arithmetics\n","print(x + 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvoZaG8NkbHx"},"source":["print(y * (x + 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zktd6b7wkgF2"},"source":["print((y * (x + 2)).exp())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2dhiDLzkyQF"},"source":["#### GPU Acceleration\n","\n","Everything can be run on a GPU\n","\n","First, let us create a [`torch.device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device) object representing a GPU device."]},{"cell_type":"code","metadata":{"id":"MPdqlb1CkmqQ"},"source":["cuda0 = torch.device('cuda:0')  # pick the GPU at index 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCYhVhc-lE7N"},"source":["# Move a tensor from CPU to GPU\n","# NOTE: the first time you access a GPU, a context is created so this may take a\n","# few seconds. But subsequent uses will be fast.\n","\n","cuda_y = y.to(cuda0)\n","print(cuda_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhhaWnPvlQ0F"},"source":["# Or directly creating a tensor on GPU\n","cuda_x = torch.zeros(3, 5, device=cuda0)\n","print(cuda_x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnaL5mNimjXo"},"source":["# All functions and methods work on GPU tensors\n","print((cuda_y * (cuda_x + 2)).exp())  # values match the CPU results above!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJb9shauw6Bu"},"source":["### NumPy Bridge\n","\n","Converting a `torch.Tensor` to a `np.ndarray` and vice versa is a breeze.\n","\n","The `torch.Tensor` and `np.ndarray` will share their underlying memory locations (if the `torch.Tensor` is on CPU and `dtype` is the same), and changing one will change the other."]},{"cell_type":"code","metadata":{"id":"wjPmAYWIxeM3"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHPeZwjPWwlx"},"source":["# convert a torch tensor into a numpy array\n","x = torch.randn(5)\n","x_np1 = x.numpy()\n","x_np2 = np.asarray(x)\n","\n","print(x)\n","print(x_np1)\n","print(x_np2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfVwkxI8x5ks"},"source":["# convert a numpy array into a torch tensor\n","\n","a = np.random.randn(3, 4)\n","a_pt = torch.as_tensor(a)\n","print(a)\n","print(a_pt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RmnP6Sayb19"},"source":["# the resulting CPU Tensor shares memory with the array!\n","# change the tensor array -> change the orignial numpy array\n","# if you want a different copy: a_pt = torch.as_tensor(a.copy())\n","\n","a_pt[0] = -1\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EAvixGtnv459"},"source":["## 1.2 Model+Loss: Flexible and Efficient Neural Network Library"]},{"cell_type":"markdown","metadata":{"id":"Ni-h3bBGz0kf"},"source":["The [`torch.nn`](https://pytorch.org/docs/stable/nn.html) and [`torch.optim`](https://pytorch.org/docs/stable/optim.html) packages provide many efficient implementations of neural network components:\n","  + Affine layers and [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n","  + Normalization methods\n","  + [Initialization schemes](https://pytorch.org/docs/stable/nn.html#torch-nn-init)\n","  + [Loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n","  + [Embeddings](https://pytorch.org/docs/stable/nn.html#sparse-layers)\n","  + [Distributed and Multi-GPU training](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)\n","  + [Gradient-based optimizers](https://pytorch.org/docs/stable/optim.html)\n","  + [Learning rate schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n","  + etc."]},{"cell_type":"markdown","metadata":{"id":"Jd11-o3xhWx4"},"source":["### Implement One-layer MLP\n","\n","We will use the [fully connected linear layer (`nn.Linear`)](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) and. \n","A fc layer performs an affine transform with a 2D weight parameter $\\mathbf{w}$ and a 1D bias parameter $\\mathbf{b}$:\n","\n","$$ f(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\mathrm{T} \\mathbf{x} + \\mathbf{b}).$$"]},{"cell_type":"markdown","metadata":{"id":"48lBcWUIhc6Y"},"source":["#### Step 1. Linear layer"]},{"cell_type":"code","metadata":{"id":"4XWfNgtI0NE0"},"source":["# all popular neural network layers\n","import torch.nn as nn \n","# handy for simple functions\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpkSxb3UvZYF"},"source":["# input x: 1D array of size 4\n","# output: 1D array of size 8\n","fc = nn.Linear(in_features=4, out_features=8)\n","print(fc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRYaIS5vzyoF"},"source":["# It has two parameters, the weight and the bias\n","# These parameters by default have `requires_grad=True`, so they will collect gradients!\n","for name, p in fc.named_parameters():\n","    print('param name: {}\\t shape: {}'.format(name, p.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKNxg_ggfaCN"},"source":["# w\n","print(fc.weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgGxaH8P0k6d"},"source":["# b\n","print(fc.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X28ALh9ihqSp"},"source":["#### Step 2. Sigmod Function (Logistic function)"]},{"cell_type":"code","metadata":{"id":"21nZqKk009u2"},"source":["# Let's construct an input tensor with 2 dimensions:\n","#   - batch dimensionsize: 2\n","#   - input size: 4\n","x = torch.randn(2, 4)\n","\n","# w*x+b\n","result_linear = fc(x)\n","# sigma(w*x+b)\n","result_logistic = F.sigmoid(result_linear)\n","print(\"linear layer output: range=[-\\infty, \\infty]\")\n","print(result_linear)\n","print(\"logistic output: range=[0, 1]\")\n","print(result_logistic)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krtyRiOdjCKv"},"source":["#### Step 3. MSE Loss Function"]},{"cell_type":"code","metadata":{"id":"HBh0R7zV7TZK"},"source":["# Say (arbitrarily) we want the layer to behave like f(x) = x^2\n","target = result_linear **2\n","\n","# Let's try MSE loss\n","loss = F.mse_loss(result_logistic, target)\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GReMBZkjjv98"},"source":["## 1.3 Optimization: Autograd and Optimizer"]},{"cell_type":"markdown","metadata":{"id":"uFpp4aL2j3tI"},"source":["#### Gradient Computation\n","PyTorch keeps track of your computations and the gradient is automatically computed!"]},{"cell_type":"code","metadata":{"id":"0HBUX4To7Zie"},"source":["# Compute gradients\n","loss.backward()\n","print(fc.bias.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZURHGQakF9Q"},"source":["#### Optimizer\n","\n","We can code up a naive optimizer with manual gradient updates like before."]},{"cell_type":"code","metadata":{"id":"5RXQNbrf7jy9"},"source":["# We can manually perform GD via a loop\n","print('bias before GD', fc.bias)\n","lr = 0.5\n","with torch.no_grad():  \n","    # this context manager tells PyTorch that we don't want ops inside to be \n","    # tracked by autograd!\n","    # o/w PyTorch will try to automatically compute the gradient of this gradient operation too.\n","    for p in fc.parameters():\n","        p -= lr * p.grad\n","        \n","print('bias after one-step GD', fc.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZq4QtJ8F5v4"},"source":["More easily, we can use the provided [`torch.optim`](https://pytorch.org/docs/stable/optim.html#torch.optim) optimizers (e.g. GD+momentum and many advanced optimizers). We will see how to use the [`torch.optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer in a second!"]},{"cell_type":"markdown","metadata":{"id":"ou68C8c-lQu4"},"source":["## Exercise 1. Redo Lab 2-Part 3 in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"v-U1tDZkn3gc"},"source":["**(a) [TODO] Data.** We will\n","- download and pre-procoss the dataset (x,y)\n","- divide them into train, val, and test sets in a 6:2:2 ratio\n","- [TODO] convert from numpy to pytorch tensor"]},{"cell_type":"code","metadata":{"id":"4ZNB9vWAlZqd"},"source":["# download data\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","url = 'https://raw.githubusercontent.com/BlohmLab/MLtutorials/week3/data/marks.txt'\n","data = pd.read_csv(url, header=None)\n","Y = np.array(data.iloc[:,-1]).astype(np.float32).reshape([-1,1])\n","\n","# by default, numpy arrays are float64, but pytorch tensor wants float32\n","X = np.array(data.iloc[:,:-1]).astype(np.float32)\n","# normalize the data for better learning\n","X = (X-X.mean(axis=0))/X.std(axis=0)\n","\n","def data_split(N, ratio=[6,2,2]):\n","  # generate a shuffle array\n","  shuffle_idx = np.arange(N)\n","  np.random.shuffle(shuffle_idx)\n","  # divide into train-val-test by the ratio\n","  data_split = (np.cumsum(ratio)/float(sum(ratio))*N).astype(int)\n","  out_idx = [None] * len(ratio)\n","  out_idx[0] = shuffle_idx[:data_split[0]]\n","  for i in range(1,len(ratio)):\n","    out_idx[i] = shuffle_idx[data_split[i-1] : data_split[i]]\n","  return out_idx  \n","\n","train_idx, val_idx, test_idx = data_split(len(Y))\n","\n","X_train, Y_train = X[train_idx], Y[train_idx]\n","X_val, Y_val = X[val_idx], Y[val_idx]\n","X_test, Y_test = X[test_idx], Y[test_idx]\n","\n","#### TODO: convert variables into pytorch tensors\n","X_train_pt, Y_train_pt = ???\n","X_val_pt, Y_val_pt = ???\n","X_test_pt, Y_test_pt = ???"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tWvTtgKbn6Nx"},"source":["**(b) [TODO] Model.** let's define the deep learning model!"]},{"cell_type":"code","metadata":{"id":"izmy6Kr3HWEE"},"source":["import torch.nn as nn \n","import torch.nn.functional as F\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        # define the layers and parameters\n","        super(LogisticRegression, self).__init__()\n","        self.linear = torch.nn.Linear(input_dim, output_dim)\n","\n","    def forward(self, x):\n","        # this function will be called to process the input data\n","        #### TODO: implement \\sigma(Wx+b)        \n","        return outputs\n","\n","# create a model (parameters are initialized)\n","# input (size = 2): two exam scores (the bias param will take care of constant term)\n","# ouput (size = 1): accept or not\n","model = LogisticRegression(input_dim = 2, output_dim = 1)\n","\n","# upload the model to GPU\n","# model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6_pIhHGpaOO"},"source":["**(c) Optimizer.** We will use all the data and SGD becomes the same as the gradient descent (GD)."]},{"cell_type":"code","metadata":{"id":"M35F6enHq2AY"},"source":["lr_rate = 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ul2Xnrcuwmo5"},"source":["**(d) Training.** We wrote the training code for you! (well, only 4 important lines!)"]},{"cell_type":"code","metadata":{"id":"XZoO55hiHMrV"},"source":["num_iter = 150\n","# training loop\n","for ii in range(num_iter):\n","    # 1. forward pass\n","    Y_hat = model(X_train_pt)\n","    \n","    # 2. compute loss\n","    loss = F.mse_loss(Y_hat, Y_train_pt)\n","    \n","    # 3. compute gradients\n","    loss.backward()\n","    \n","    # 4. gradient update\n","    optimizer.step()\n","    \n","    # add some printing\n","    if ii % 10 == 0:\n","        print('iteration {}\\tloss {:.5f}'.format(ii, loss))\n","\n","# if the torch tensor has \"require_grad\", need to detach it first\n","ww = model.linear.weight.detach().numpy()[0]\n","bb = model.linear.bias.detach().numpy()[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cMpXuc2l0mnn"},"source":["**(e) Evaluation.** We will first compute the accuray and plot the predicted decision boundary. Note that the model that fits the training data well, may still have much error on the val and test data."]},{"cell_type":"code","metadata":{"id":"F7BKsh860m1a"},"source":["def compute_accuracy(model, x=X, y=Y):\n","    \"\"\"function that compares predicted y to true y and returns accuracy\"\"\"\n","    y_pred = model(x)>0.5\n","    accuracy = (y_pred == y).sum()/len(y)\n","    return accuracy \n","\n","print('Train acc:', compute_accuracy(model, X_train_pt, Y_train_pt))\n","print('Val acc:', compute_accuracy(model, X_val_pt, Y_test_pt))\n","print('Test acc:', compute_accuracy(model, X_val_pt, Y_test_pt))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPLsGQfTwDq-"},"source":["def plot_logistic_regression(fig, param_W, param_b, x=X, y=Y, subp=111, title='train'):\n","  # plot original data\n","  X_admitted = x[y==1,:]\n","  X_rejected = x[y==0,:]\n","  \n","  ax = fig.add_subplot(subp)\n","  ax.scatter(X_admitted[:,0],X_admitted[:,1])\n","  ax.scatter(X_rejected[:,0],X_rejected[:,1])\n","  ax.set_xlabel('Mark 0')\n","  ax.set_ylabel('Mark 1')\n","  ax.legend(('Accept','Reject'))\n","\n","  # plot the decision boundary\n","  xx = np.linspace(-2, 2,100)\n","  yy = -param_W[0]/param_W[1]*xx - param_b/param_W[1]\n","  ax.plot(xx,yy,'g-')\n","  plt.title(title)\n","  \n","\n","fig = plt.figure()\n","plot_logistic_regression(fig, ww, bb, X_train, Y_train[:,0], 131, 'train')\n","plot_logistic_regression(fig, ww, bb, X_val, Y_val[:,0], 132, 'val')\n","plot_logistic_regression(fig, ww, bb, X_test, Y_test[:,0], 133, 'test')\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bPxidAOAFPlQ"},"source":["# Part 2. Polynomial Regression (Exercise 2)\n","\n","Linear regression itself can be simple, but it can be used together with hand-crafted features and become a bit more powerful. \n","As the solution can be computed with an analytical closed-form, **we will just use numpy library**."]},{"cell_type":"markdown","metadata":{"id":"oyDNuSv4F23v"},"source":["**(a) Data.** We use a 2nd-order polynomial function and Gaussian noise to generate the ground truth regression values."]},{"cell_type":"code","metadata":{"id":"QCOg_avwF3A9"},"source":["import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#np.random.seed(int(time.time()))\n","np.random.seed(123)\n","\n","# gt polynomial: (x-5)^2\n","num_pt = 10\n","\n","X = np.random.uniform(4, 10, num_pt).reshape(-1,1)\n","Y = (X - 5)**2 + 0.5 *np.random.normal(0, 1, num_pt).reshape(-1,1)\n","\n","# for visualization\n","XX = np.linspace(4,10,100).reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEbRqdbtINdG"},"source":["def data_split(N, ratio=[6,2,2]):\n","  # generate a shuffle array\n","  shuffle_idx = np.arange(N)\n","  np.random.shuffle(shuffle_idx)\n","  # divide into train-val-test by the ratio\n","  data_split = (np.cumsum(ratio)/float(sum(ratio))*N).astype(int)\n","  out_idx = [None] * len(ratio)\n","  out_idx[0] = shuffle_idx[:data_split[0]]\n","  for i in range(1,len(ratio)):\n","    out_idx[i] = shuffle_idx[data_split[i-1] : data_split[i]]\n","  return out_idx  \n","\n","train_idx, val_idx, test_idx = data_split(len(Y))\n","\n","X_train, Y_train = X[train_idx], Y[train_idx]\n","X_val, Y_val = X[val_idx], Y[val_idx]\n","X_test, Y_test = X[test_idx], Y[test_idx]\n","\n","\n","def plot_data(split = 'train', subp=111):\n","  plt.subplot(subp)\n","  if split == 'train':\n","    plt.plot(X_train, Y_train, 'r.')\n","  elif split == 'val':\n","    plt.plot(X_val, Y_val, 'bx')\n","  elif split == 'test':\n","    plt.plot(X_test, Y_test, 'go')\n","  plt.xlim([4,10])  \n","  plt.title(split)\n","\n","plot_data()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90h9Ol2tIrHK"},"source":["**(b) [TODO] K=1:** Implement the linear regression.\n","\n","**Course material: Lecture 7, page 18**"]},{"cell_type":"code","metadata":{"id":"xgxgkUl2IlhT"},"source":["def MSE(y, y_hat):\n","  # MSE loss to measure the regression distance\n","  return ((y-y_hat)**2).mean()\n","\n","\n","def train_LR(x,y):\n","  #### TODO: complete the analytical solution for linear regression  \n","  return ???\n","\n","# concatenate the all one array for the bias term\n","X1_train = np.hstack([X_train, np.ones([X_train.shape[0],1])])\n","f_K1 = train_LR(X1_train, Y_train)\n","mse_K1_train = MSE(f_K1(X1_train), Y_train)\n","\n","X1_val = np.hstack([X_val, np.ones([X_val.shape[0],1])])\n","mse_K1_val = MSE(f_K1(X1_val), Y_val)\n","\n","\n","X1_test = np.hstack([X_test, np.ones([X_test.shape[0],1])])\n","mse_K1_test = MSE(f_K1(X1_test), Y_test)\n","\n","print('K=1: train-MSE=%.2f' % mse_K1_train)\n","print('K=1: val-MSE=%.2f' % mse_K1_val)\n","print('K=1: test-MSE=%.2f' % mse_K1_test)\n","\n","XX1 = np.hstack([XX, np.ones([XX.shape[0],1])])\n","plot_data('train', 131)\n","plt.plot(XX, f_K1(XX1), 'k-')\n","plot_data('val', 132)\n","plt.plot(XX, f_K1(XX1), 'k-')\n","plot_data('test', 133)\n","plt.plot(XX, f_K1(XX1), 'k-')\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1QgKi_ZHO2Pt"},"source":["**(c) [TODO] K=2:** Implement a polynomial regression function. It can be seen as first create features from input X and then apply linear regression\n","\n","**Course material: Lab 3, page 23**"]},{"cell_type":"code","metadata":{"id":"SJSlNpAlKpVy"},"source":["def LR_K2(x, y=None, f=None):\n","  #### TODO: complete the feature matrix\n","  x_feat = ???\n","  if f is None:\n","    f = train_LR(x_feat, y)\n","  if y is None: # test-time value\n","    return f(x_feat)\n","  mse = MSE(f(x_feat), y)\n","  return f, mse\n","\n","# train the model\n","f_K2, mse_K2_train = LR_K2(X_train, Y_train)\n","# evaluate the model\n","\n","_, mse_K2_val = LR_K2(X_val, Y_val, f_K2)\n","_, mse_K2_test = LR_K2(X_test, Y_test, f_K2)\n","\n","\n","print('K=2: train-MSE=%.2f' % mse_K2_train)\n","print('K=2: val-MSE=%.2f' % mse_K2_val)\n","print('K=2: test-MSE=%.2f' % mse_K2_test)\n","\n","plot_data('train', 131)\n","plt.plot(XX, LR_K2(XX, f=f_K2), 'k-')\n","plot_data('val', 132)\n","plt.plot(XX, LR_K2(XX, f=f_K2), 'k-')\n","plot_data('test', 133)\n","plt.plot(XX, LR_K2(XX, f=f_K2), 'k-')\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bDukdlkPPzzU"},"source":["**(d) [TODO] Arbitrary K:** Implement a polynomial regression function with an arbitrary number K.\n","**Course material: Lab 3, page 23**"]},{"cell_type":"code","metadata":{"id":"DsyWxGQ_Pz70"},"source":["def LR_KK(x, y=None, K=2, f=None):\n","  x_feat = [None] * (K+1)\n","  for i in range(K+1):\n","    #### TODO: complete each column in the feature matrix\n","    x_feat[i] = ???\n","  x_feat = np.hstack(x_feat)\n","\n","  if f is None:\n","    f = train_LR(x_feat, y)\n","  if y is None: # test-time value\n","    return f(x_feat)\n","  mse = MSE(f(x_feat), y)\n","  return f, mse\n","\n","# Let K=10\n","# train the model\n","f_K10, mse_K10_train = LR_KK(X_train, Y_train, 10)\n","# evaluate the model\n","\n","_, mse_K10_val = LR_KK(X_val, Y_val, 10, f_K10)\n","_, mse_K10_test = LR_KK(X_test, Y_test, 10, f_K10)\n","\n","\n","print('K=10: train-MSE=%.2f' % mse_K10_train)\n","print('K=10: val-MSE=%.2f' % mse_K10_val)\n","print('K=10: test-MSE=%.2f' % mse_K10_test)\n","\n","plot_data('train', 131)\n","plt.plot(XX, LR_KK(XX, K=10, f=f_K10), 'k-')\n","plot_data('val', 132)\n","plt.plot(XX, LR_KK(XX, K=10, f=f_K10), 'k-')\n","plot_data('test', 133)\n","plt.plot(XX, LR_KK(XX, K=10, f=f_K10), 'k-')\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PAiwqhOLUI4d"},"source":["(e) Plot generalization error"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"-_B7NKxAUHkq","executionInfo":{"status":"ok","timestamp":1632730666377,"user_tz":240,"elapsed":659,"user":{"displayName":"Donglai Wei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64","userId":"05000800795689376079"}},"outputId":"66fae01f-46cc-47a3-8626-ed52acbb5e72"},"source":["Ks = range(1, 11)\n","train_err = np.zeros(len(Ks))\n","test_err = np.zeros(len(Ks))\n","for i,K in enumerate(Ks):\n","  f, train_err[i] = LR_KK(X_train, Y_train, K)\n","  _, test_err[i] = LR_KK(X_test, Y_test, K, f)\n","\n","\n","\n","plt.plot(Ks, train_err,'b-')\n","plt.plot(Ks, test_err,'r-')\n","plt.legend(('train err','test err'))\n","plt.xlabel('regression order K')\n","plt.ylabel('MSE')\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3daZhU5bX28f+ioWUQAQEVBISoIIiIDArBLkEiDhg1idEYSWIcUGMUE4foMYNJjomJQxRfJxQSjcZ4QvQ4xES0GkQFRUBUJmkHJuFIi8okYAPP+2FV0QPddNNd1buG+3ddddW89+pS7tq19rOfbSEEREQk9zSJugAREUkPBbyISI5SwIuI5CgFvIhIjlLAi4jkqKZRF1BRhw4dQvfu3aMuQ0Qka8yZM+eTEELH6p7LqIDv3r07s2fPjroMEZGsYWbLanpOLRoRkRylgBcRyVEKeBGRHJVRPfjqlJWVsXLlSrZs2RJ1KRmvefPmdOnShWbNmkVdiohkgIwP+JUrV9K6dWu6d++OmUVdTsYKIbB27VpWrlxJjx49oi5HRDJAxrdotmzZQvv27RXutTAz2rdvr186IrJTxgc8oHCvI31OIlJRVgS8iEhKhQDTp8M990BZWdTVpI0Cvhaff/4599xzT73ee8opp/D555+nuCIRqbePPoLf/Q569oTjjoPLLoOvfx02bIi6srRQwNdidwG/bdu23b73ueeeo23btimtp+o6a6tBJO9t3QqTJ8Mpp0C3bnDDDdClCzz8MNx7L7z4IsRisGpV1JWmXMaPoonaddddx/vvv0///v054YQTGD16NL/4xS9o164dixcvZsmSJZxxxhmsWLGCLVu2MG7cOMaOHQuUT72wceNGTj75ZI499lhmzJjBgQceyFNPPUWLFi0qrau0tJRLLrmE5cuXA3DHHXcwbNgwbrzxRt5//30++OADunXrRq9evSrdf+yxxxr9cxHJeG+/DZMmwSOPwNq1Hur/9V9w3nlw8MHlrzvoIPj2t2HoUHjuOTj88MhKTrWsCvgrr4R581K7zP794Y47an7+5ptvZv78+cxLrHjatGnMnTuX+fPn7xyOOGnSJPbdd182b97M4MGD+da3vkX79u0rLaekpITHHnuMBx54gLPOOot//vOfjBkzptJrxo0bx09+8hOOPfZYli9fzoknnsiiRYsAWLhwIa+88gotWrTgxhtvrHRfRBI++wwee8yDfc4cKCyEM86A88+Hr30NCgp2fc/JJ3s/fvRoGDYM/vd/YfjwRi89HbIq4DPF0UcfXWms+fjx43nyyScBWLFiBSUlJbsEfI8ePejfvz8AAwcOZOnSpbss98UXX2ThwoU7769fv56NGzcCcNppp1UK86r3RfLWjh1QXOyh/sQT3pI58kgYPx6++12o8m+xWgMGwGuvedifeCL85S9wzjlpLz3dsirgd7el3ZhatWq18/a0adN48cUXmTlzJi1btmT48OHVjkXfa6+9dt4uKChg8+bNu7xmx44dvPbaazRv3ny366zuvkjeWbrUg/gvf4Fly6BdO7joIt9aP+qoPV/eQQfBq6/6Fv93vwvLl8O110IWDz/WTtZatG7dmg272cO+bt062rVrR8uWLVm8eDGvvfZavdc1atQo7rrrrp3356W6HyWS7TZvhr/9zdstPXrAb34DvXrB3//uO0nvuqt+4Z7Urh1MmQLf+Q5cd52Pstm+PXX1NzIFfC3at2/PsGHD6Nu3L9dcc80uz5900kls27aN3r17c9111zFkyJB6r2v8+PHMnj2bfv360adPH+67776GlC6SG0KA2bPhRz+CTp3g3HPhgw883Jcuheefh7PPhmp++dbLXnvBo4/61vu998I3vgGbNqVm2Y3MQghR17DToEGDQtUTfixatIjevXtHVFH20eclOaO01IN20iR45x0P8DPP9BbMccdBk0bYPr37brjiChg4EJ59FvbbL/3r3ENmNieEMKi657KqBy8iOW7bNm+RTJoETz/tR5kefTTcd5+3Tdq0adx6LrvMh1eec44Po/z3v/0gqSyhFo2IRK+kxMeoH3SQD1ecPh0uv9y33F9/HS6+uPHDPen002HqVD/a9atfhRkzoqmjHhTwIhKNjRt9BEws5lvFf/iDD1d84glYuRJuuw369o26SnfMMTBzpu+EHTnSa8wCCngRaTwheFBeeKHvMP3hD+Hjj+Hmm2HFCnjmGd+pWVgYdaW7Ovhgr/2oo3xfwJ13Rl1RrdSDF5H0W7MG/vpXmDgRFi2CVq185Mv553vbI1vGmnfoAPG4j+S58koff3/rrY2zw7ceFPAikh7bt/sO04kT4amnfAfq0KHw4INw1lnQunXUFdZPixbwj3/AT38Kf/qT//L4619TN0wzhTLzayeDNGS6YPAJw7744osUViSS4T78EH75S+je3WdwfOklH2q4YIHvoLzgguwN96SCAj+0/rbbfKbKr33NJzTLMAr4WkQd8JoeWLLCli0+ydfXvgZf+Qr893/7DtLJk30O9ttugz59oq4ytcx8K/5//scPxBo2zA/AyiAK+FpUnC44eSTrLbfcwuDBg+nXrx+/+tWvANi0aROjR4/myCOPpG/fvjz++OOMHz+eVatWMWLECEaMGLHLsufMmcNxxx3HwIEDOfHEE1m9ejUAw4cP58orr2TQoEHceeedu9wXyRhvveVb5507+/wt77/vR5guW+Zjxr/1rczcYZpK3/62zym/Zo23oN54I+qKdsquHnwE8wVXnS54ypQplJSUMGvWLEIInHbaaUyfPp3S0lI6d+7Mv/71L8DnqGnTpg233347U6dOpUOHDpWWW1ZWxuWXX85TTz1Fx44defzxx7nhhhuYNGkSAF9++SXJo3qfeeaZSvdFIrVunW+tT5zoW66FhfDNb3rr5fjjM3aHY1ode6y3n04+2acafvxxOPXUqKvKsoDPAFOmTGHKlCkclZjQaOPGjZSUlFBUVMRVV13Fz372M0499VSKiop2u5x3332X+fPnc8IJJwCwfft2OnXqtPP5s88+u9Lrq94XaVQhwMsve6j/4x8+6Ve/fj5U8Nxz6zYlb6477DAfRnnqqX5w1N13wyWXRFpSdgV8BswXHELg+uuv5+KLL97lublz5/Lcc8/x85//nJEjR/LLX/5yt8s5/PDDmTlzZrXPa3pgyQirV8NDD/nUASUlsM8+8P3v+zj2gQOzZ3hjYzngAJg2zYeAXnqpt6puuimyXzV5+Ftqz1SdLvjEE09k0qRJO0/E8dFHH7FmzRpWrVpFy5YtGTNmDNdccw1z586t9v1JvXr1orS0dGfAl5WVsWDBgkb4i0RqsW2bzwNz+unQtStcf70flPTQQx74990HgwYp3Guy994+LHTsWD+A63vf85OQRCC7tuAjUHG64JNPPplbbrmFRYsWMXToUAD23ntvHnnkEd577z2uueYamjRpQrNmzbj33nsBGDt2LCeddBKdO3dm6tSpO5dbWFjI5MmTueKKK1i3bh3btm3jyiuv5PAcOh+kZJmSEt9STwb5AQfA1Vf7wUhZNMFWRmja1L8Iu3f3OXZWrYInn4S2bRu1jLRPF2xmBcBs4KMQwm73Omi64IbT5yV75Isv4J//9N76Sy/5+O5TTvEdpqecAs2aRV1h9nvkkfIvyeeeg27dUrr43U0X3BgtmnHAokZYj4jURQh+QupLL/XWy/e/72PVf/97P01dsj2jcE+NMWPgP//xI16HDk39SMDdSGuLxsy6AKOBm4CfpnNdIlKDELzlsmgRvPmmb1G+9ZYfWv/tb/vWeiymnno6HX+8n+/15JOhqMh/NY0alfbVprsHfwdwLVDjcclmNhYYC9Cthp8uIQRM//PVKpPOziUR2LHDR20sWgQLF1a+Xreu/HUDB8I99/hJLBq5J5zX+vaF117z1tfo0fDAA3DeeWldZdoC3sxOBdaEEOaY2fCaXhdCmABMAO/BV32+efPmrF27lvbt2yvkdyOEwNq1a2megRMeSYqVlfkRo1VDfPFiH5+etP/+0Lu3j1Pv3dsvffp4W0aiceCBfjzBmWf6VMnLlvm8PWnKtnRuwQ8DTjOzU4DmwD5m9kgIYcyeLKRLly6sXLmS0tLStBSZS5o3b06XLl2iLkNSZfNmWLJk1yAvKfGQT+rWzcN7+PDyEO/dG/bdN7LSZTf22Qf+9S+46CK48UYP+fvvT8s+j7QFfAjheuB6gMQW/NV7Gu4AzZo1o0ePHimuTiSDrF/vwV0xxBct8omrkm23Jk38hBO9e8Npp5UH+WGH+bhryS7NmsGf/+ynKPzNb3wn95NPQsuWKV2NxsGLNJZPPtl1a3zhQv/HnVRY6MPpBg700RfJrfFDD83I+calAczg17/2X2AvvJCW/75pHwe/J6obBy+StT75xGcZnDLF/wGvXFn+XKtWvvWdDPDk9Ve+4gfJSH4Jod59+N2Ng9f/SSKp8uWXPtnUlCnw/PMwd67/w23b1udJHzq0PMi7ds3PWRelelm4k1Ukt4XgOzyTgT51Kmza5EeDDhniP79HjfJ5WwoKoq5W8pACXmRPfPYZFBeXh/qyZf74V77iR4SOGgUjRkCbNtHWKYICXmT3tm2DWbPKA33WLD+gqHVrGDkSfvYzD/WDD466UpFdKOBFqvrwQw/zKVMgHvdhjE2awODBcMMNHujHHKO5WiTjKeBF1q/3/vmUKX557z1/vGtXOOssD/SRI3XgkGQdBbzkn+3bfTbFZKDPnOmtmJYtvX9++eVw4ok+Hl3TY0gWU8BLflixojzQX3wRPv3UHx8wwE9qceKJPoxxr72irVMkhRTwktueecZ3hC5KnJKgc2c/1H/UKB+b3rFjtPWJpJECXnLb9df7pF233eahfvjhartI3lDAS+76+GNYsMDPVPRTnW9G8o+OlZbcVVzs1yNHRluHSEQU8JK7iov9iNIBA6KuRCQSCnjJXfG4nwRD88BInsr6gN+6Ff7wBx/9JrLThx/6Re0ZyWNZH/CFhXDrrfDoo1FXIhkl2X8//vho6xCJUNYHvBnEYjB9etSVSEaJx+GAA3z+dZE8lfUBD1BUBEuX+sGKIoTgW/DHH68x75LXciLgYzG/fvnlaOuQDLFwoY+BV3tG8lxOBPyRR/r03GrTCODtGdAOVsl7ORHwBQVw7LEKeEkoLoYePaB796grEYlUTgQ8eJtm0SIoLY26EonUtm0wbZq23kXIsYAHeOWVaOuQiL35Jqxbp4AXIYcCftAgaN5cbZq8l+y/jxgRbR0iGSBnAr6wEIYMUcDnvXgc+vaF/fePuhKRyOVMwIO3aebN81/okoe2bvUenYZHigA5GPA7dsCMGVFXIpGYORO2bFH/XSQhpwJ+yBBo2lQHPOWt4mJo0gSOOy7qSkQyQk4FfKtWMHCg+vB5Kx73ve1t2kRdiUhGyKmAB2/TzJrlp+GUPLJhg/+HV3tGZKecDPiyMnj99agrkUb18st+kJN2sIrslHMBP2yYTyCoNk2eicdhr738fwARAXIw4Nu1g379tKM17xQXw1e/Ci1aRF2JSMbIuYAHnx9+xgxv1Uge+OQTPwBC7RmRStIW8GbW3MxmmdlbZrbAzH6drnVVFYvBF1/A3LmNtUaJ1NSpfq0drCKVpHMLfitwfAjhSKA/cJKZDUnj+nYqKvJr9eHzRHEx7L23D5EUkZ3SFvDBbUzcbZa4hHStr6IDDoCePRXweSMe94ObmjWLuhKRjJLWHryZFZjZPGAN8EIIYZfBi2Y21sxmm9ns0hRO5h6L+bQkO3akbJGSiVasgJIStWdEqpHWgA8hbA8h9Ae6AEebWd9qXjMhhDAohDCoY8eOKVt3URF8/jnMn5+yRUomKi72a+1gFdlFo4yiCSF8DkwFTmqM9UH5CUDUpslx8Th06ABHHBF1JSIZJ52jaDqaWdvE7RbACcDidK2vqoMOgq5dFfA5LQTfgh8xwicZE5FKmqZx2Z2Ah8ysAP8i+Z8QwrNpXF8lZr4V/+KLngNmjbVmaTRLlsBHH6n/LlKDtAV8COFt4Kh0Lb8uYjF49FHfB9ezZ5SVSFok++8KeJFq5fTv2mQfXtMW5Kh43PtwBx8cdSUiGSmnA75XL+jYUX34nLRjhx/BOnKk+m8iNcjpgDfz4ZIK+Bz01lvw6acaHimyGzkd8OBtmqVLYfnyqCuRlIrH/Vr9d5Ea5UXAg/rwOae4GA47DDp3jroSkYyV8wHfrx/ss48CPqd8+aX33dSeEdmtnA/4ggI/yY/68Dlk1izYtEntGZFa5HzAg7dpFi2CNWuirkRSorjY96APHx51JSIZLW8CHnx2SckB8TgcdRTsu2/UlYhktLwI+EGDoHlztWlywhdfwMyZas+I1EFeBHxhIQwdqh2tOeGVV/xku9rBKlKrvAh48AOe5s2DdeuirkQaJB6Hpk3Lz8soIjXKm4CPxfzo9hkzoq5EGqS4GIYMgVatoq5EJOPlTcAPGeIbfurDZ7HPPoM5c9R/F6mjvAn4Vq18Z6sCPou99JJP7q+AF6mTvAl48DbNG2/A5s1RVyL1Eo9Dy5ZwzDFRVyKSFXYb8GY2psLtYVWe+3G6ikqXoiIfgPH661FXIvUSj/t/xMLCqCsRyQq1bcH/tMLtu6o8d36Ka0m7YcP8AEi1abLQ6tV+OLKGR4rUWW0BbzXcru5+xmvXzicfU8BnIZ2eT2SP1RbwoYbb1d3PCrGYHwhZVhZ1JbJHiov9G7p//6grEckatQX8YWb2tpm9U+F28n6vRqgv5WIxP9p97tyoK5E6C8H778OH+/SgIlInTWt5vnejVNGIkgdATp+uwRhZ44MPYNkyuOaaqCsRySq73YIPISyreAE2AgOADon7WWf//aFnT/Xhs4r67yL1UtswyWfNrG/ididgPj565q9mdmUj1JcWsZhPPLZ9e9SVSJ3E49CpE/TKyq6gSGRq68H3CCHMT9z+IfBCCOHrwDFk4TDJpFjMJx2bP7/210rEQvAt+JEjfYyriNRZbQFfcazJSOA5gBDCBmBHuopKN52IO4vMnw+lpRr/LlIPtQX8CjO73My+gffe/wNgZi2AZukuLl0OOgi6dVMfPivE436t/rvIHqst4C8ADgfOA84OIXyeeHwI8Oc01pV2RUUe8CErR/PnkeJiOOQQ/0YWkT1S2yiaNSGES0IIp4cQplR4fGoI4db0l5c+sRh8/DGUlERdidRo2zafQVLtGZF62e04eDN7enfPhxBOS205jSfZh58+3YdNSgaaMwfWr1d7RqSeajvQaSiwAngMeJ0snH+mJr16QceOvqP1wgujrkaqley/jxgRbR0iWaq2gD8AOAE4B/gu8C/gsRDCgnQXlm5mvhWvHa0ZLB732eE6doy6EpGsVFsPfnsI4T8hhB/gO1bfA6Zl41zw1SkqgqVLYfnyqCuRXWzZAq++qvaMSAPUekYnM9vLzL4JPAJcBowHnkx3YY1B4+Ez2IwZsHWrdrCKNEBtUxU8DMzEx8D/OoQwOITw2xDCR7Ut2My6mtlUM1toZgvMbFyKak6Zfv1gn33UpslIxcU+c2TyW1hE9lhtPfgxwCZgHHCFlR8qbkAIIeyzm/duA64KIcw1s9bAHDN7IYSwsKFFp0pBARx7rAI+I8XjMHiwfwOLSL3U1oNvEkJonbjsU+HSupZwJ4SwOoQwN3F7A7AIODB1padGLAaLF8OaNVFXIjutX+9nR1f/XaRBau3Bp4KZdQeOwodaVn1urJnNNrPZpaWljVFOJcn54V95pdFXLTWZPt2n+lTAizRI2gPezPYG/glcGUJYX/X5EMKEEMKgEMKgjhEMhxs0CJo3V5smo8Tj/h9l6NCoKxHJamkNeDNrhof7oyGEJ9K5rvoqLPQcUcBnkOJiGDbMQ15E6i1tAW++R3YisCiEcHu61pMKsRjMm+dzxEvE1qyBt9/W8EiRFEjnFvww4HvA8WY2L3E5JY3rq7dYzGeVnDEj6kqEqVP9Wv13kQarbZhkvYUQXiFL5q4ZMgSaNvU2zcknR11Nnisu9qGRAwdGXYlI1muUUTSZrmVL39mqPnwGiMfhuOP8G1dEGkQBnxCL+dDrL76IupI8tmwZvP++2jMiKaKAT4jFoKwMXt9lpL40muJiv9YOVpGUUMAnDBvmUwhr4rEIxeOw337Qt2/UlYjkBAV8Qtu2cOSR6sNHJgTfgj/+eP+mFZEGU8BXUFTkQyW//DLqSvLQ4sWwerXaMyIppICvIBaDzZth7tyoK8lDyf67drCKpIwCvoLkxGNq00QgHoeDDoIePaKuRCRnKOAr2H9/Pxm3drQ2su3bYdo033pX/10kZRTwVcRiHvDbt0ddSR6ZNw8++0ztGZEUU8BXUVTkk47Nnx91JXkkHvfrESOirUMkxyjgq0ieAlR9+EZUXAx9+kCnTlFXIpJTFPBVHHQQdOumgG80X37pPTENjxRJOQV8NZJ9+BCiriQPvP66TwCk/rtIymV/wK9fD2efDZMnp2yRsRh8/DGUlKRskVKTeByaNIHhw6OuRCTnZH/At2wJ770Hl10Ga9emZJEaD9+I4nEYMMDnihCRlMr+gG/aFCZNgk8/hZ/+NCWL7NULOnZUwKfdpk3w2mtqz4ikSfYHPPgsYdddBw8/DP/+d4MXZ+ZtGgV8mr38Mmzbph2sImmSGwEP8POfQ+/ecPHFsGFDgxcXi/n5J5YvT0FtUr3iYigshGOPjboSkZyUOwG/114wcSKsXOlb8w2UHA+vaQvSKB6HoUN9P4qIpFzuBDx4WFxxBdxzT4P7K0cc4ed+VpsmTT79FN58U+0ZkTTKrYAHuOkmn5Hwwgt97t96KijwzoECPk2mTfMDDbSDVSRtci/gW7WCBx7wQew33tigRcVifh6KNWtSU5pUEI/7f6vBg6OuRCRn5V7Ag28VXnAB3HorzJ5d78Uk+/CvvJKiuqRccbF/wIWFUVcikrNyM+DBw33//T3o63kOvoEDoUULtWlS7qOP/KeR2jMiaZW7Ad+2Ldx7L7z9Nvzxj/VaRGGh77dVwKdY8vR82sEqkla5G/AAp5/u89T89rewcGG9FlFU5OejWLcuxbXls+Ji2HdfP0BNRNImtwMeYPx4aN0azj+/XqdpisV8sMerr6ahtnwUgu9gHTHCJxkTkbTJ/X9h++0Hd97p09Leddcev33IEJ/uRm2aFHn/fVixQv13kUaQ+wEP8N3vwujRcMMN8MEHe/TWli19JJ+OaE2R5On5FPAiaZcfAW/mO1wLCuCii/b4TB6xGLzxhp+XQhooHocDD4RDD426EpGclx8BD9C1K9xyi+/gmzhxj95aVARlZd7lkQbYsQOmTvWtd7OoqxHJefkT8OBb78OHw1VX+VjsOho2zPNIffgGeucd+OQTDY8UaST5FfBNmvg0BmVlcOmldW7VtG3rI/oU8A2UHP+u/rtIo0hbwJvZJDNbY2bz07WOejnkEB8X/8wz8PjjdX5bLAYzZ9b7oFgB77/37AldukRdiUheSOcW/F+Ak9K4/PobN86Hxlx+ubcM6iAW88kp585Nc225qqwMXnpJ7RmRRpS2gA8hTAc+TdfyGyR5Htd16zzs6yB50iG1aepp9mzYuFHtGZFGFHkP3szGmtlsM5tdWlraeCvu29fHxf/tb/Dss7W+fP/9/WTcCvh6So5/Hz480jJE8knkAR9CmBBCGBRCGNSxY8fGXfn113vQX3JJnSabicV86uB6zHggxcXQvz906BB1JSJ5I/KAj1RhoY+JX70arr221pfHYv49MD+zdhtnvs2bYcYMtWdEGll+BzzA0UfDT34CEyb4QTi7kTwBiNo0e+jVV2HrVu1gFWlk6Rwm+RgwE+hlZivN7IJ0ravBfvMbOPhgPxBqN/MRdOvmFwX8Hiou9h3bRUVRVyKSV9I5iuacEEKnEEKzEEKXEMKezQ/QmFq2hAcf9JkOf/nL3b40FvOA38PpbPJbPO6/lFq3jroSkbyiFk3S8OFw8cXwpz/BrFk1viwW85NwL1nSeKVltXXrfIik+u8ijU4BX9Ef/widO/vJQWo4ZDXZh9f0wXX00ks+yZgCXqTRKeAr2mcfuO8+WLAAfve7al/Ss6efQ0R9+DqKx/3M5UOGRF2JSN5RwFc1ejScey7cdJPPfliFWXkfXuqguNgPA95rr6grEck7Cvjq3HEHtGvnrZpt23Z5uqgIli3zi+zGxx/7QQMaHikSCQV8dTp08PO3zp7tYV+F+vB1lDyuQP13kUgo4Gty1llw+unwi1/Ae+9VeuqII6BNGwV8reJx/6AGDIi6EpG8pICviRncc4/3ji+80EeCJBQUeFtZffhaxOM+/LSgIOpKRPKSAn53OneG227zoX4TJlR6KhaDxYt9TLxU48MP/aL2jEhkFPC1Of98D6lrr4UVK3Y+nDzqXm2aapSW+gnOQQEvEiEFfG3M/Dyu27f7tMKJOQoGDvTh3WrTJIQA06bBOef4KfnuvRfOOAN69466MpG8pYCvix49/MCn557zE4TgMw0PHaoteNauhdtv9yAfMQL+/W+f8mH+fHjySf+CFJFIKODr6sc/9kQfN25n4z0Wg3nz6nSukNwSgn+zjRkDBx4IV13lxw38+c+wahWMHw+HHx51lSJ5TwFfVwUFPuPkhg1wxRWAB3wIPt15XvjsMw/vvn39j3/mGbjgAnjrLZg5E847z2fmFJGMoIDfE336+Lj4xx+Hp57imGOgWbMc78OHUB7enTv7L5hWrfzLbtUquPtu6Ncv6ipFpBoWMmhi80GDBoXZs2dHXcbulZXB4MHeplm4kK+e0hbwM9LllHXr4JFH4P77fU6evff2OXouvhiOOirq6kQkwczmhBAGVfectuD3VLNmMGmSB/zVVxOL+YwGuzkRVPYIwefCv+AC31r/8Y/9773/ft9av+8+hbtIFlHA18eAAXD11TBxIt9o/SJlZfD661EX1QAbNnh4DxgAxxwDf/+7D3d84w2YMwfGjtXZmESykAK+vn71K+jZk8ETLqIVm7KzD58M706d4NJLfTqGu+/2rfUHH4RB1f7qE5Es0TTqArJWixbw4IM0icW4v8MNTJq+66yTGWnjRt9Cv/9+7y21aCHZBOIAAAsVSURBVAFnn+299WOO0bh1kRyiLfiGKCqCyy7jnE/Gs/2VmTWd5S8zvPUW/OhH3lu/6CLYvNmHPK5a5ePXhwxRuIvkGAV8Q/3+92zu0JV7vryAuTO3Rl1NZV98UR7e/fv7zuEzzoBXXvGRMZdfDm3bRl2liKSJAr6hWrem7P9NoA+L2PGb3zbOOkPwuXHKymDLFti0yXeUfv65Tx3w1lse3skTiK9bB3/6k2+tP/wwDBumrXWRPKAefAq0PftEnhz7A06bejOMnOnhu2OHXycvFe839HaFuelrVFgIZ57pvfWiIgW6SB5SwKfI9DNup8nfNnLa1o+xgibQtKmHbEGBX5o0abzbe+8Np53mpx4UkbylgE+RgSfsyxkPT+aG4eW5mtxornq9u+dS8dpmzeAr78KhO6BjR228i+QrBXyKjBzp82zddFPUlVTWpg0ceij07OmX5O1DD/XnRCR3KeBTpFMn37+5ZcvOc4IA5bdruk7Ha7ZsgfffhyVLoKTEr2fMgMceq/z6/fbbNfR79oRDDvHh8SKS3TTZWB5JBn8y9JPXS5bA//1f5dd27Vo59JO3e/TwFpCIZIbdTTamLfg80ry5n4ejunNxbNjggV8x9EtKfGbkzz4rf11BgYd8dS2frl19X6+IZAYFvAA+l9iAAX6pau3aXbf4S0r8FKwVZ9Fs3tzbOxW3+vv08cs++zTanyIiCQp4qVX79n62wqFDKz8eAqxeXTn0lyyBxYvh2Wf9OKykLl3Kw/7ww8tv60BakfRRwEu9mfnBsp07w/DhlZ/btg2WLoVFi2DBAli40C8TJlTe6u/UqXLgJ78A9t23Mf8SkdykgJe0aNrU2zWHHAJf/3r54zt2wLJl5YG/cKF/AUyc6DMuJO2//66h36ePj+sXkbpJa8Cb2UnAnUAB8GAI4eZ0rk8yX5MmvpO2Rw8YPbr88RBgxYrKob9wIfz1r7B+ffnrOnTYtc3Tp49/IeiALpHK0hbwZlYA3A2cAKwE3jCzp0MIC9O1TsleZtCtm19OOqn88RB8jrSKbZ6FC31M/+efl79u330rB37yS6BTJwW/5K90bsEfDbwXQvgAwMz+DpwOKOClzszgwAP9MmpU+eMh+Nj9qlv8kyfDp5+Wv65NG99HoJCXTNa+PWk5K1w6A/5AYEWF+yuBY6q+yMzGAmMBunXrlsZyJJeY+dZ5p04+TURSCFBaWh76Cxb4fZFMlq7RZJHvZA0hTAAmgB/JGnE5kuXMfAqG/fbbdWSPSL5J53GHHwFdK9zvknhMREQaQToD/g3gUDPrYWaFwHeAp9O4PhERqSBtLZoQwjYz+zHwPD5MclIIYUG61iciIpWltQcfQngOeC6d6xARkepp7j8RkRylgBcRyVEKeBGRHKWAFxHJURl1yj4zKwWWRV1HA3UAPom6iAyhz6IyfR6V6fMo15DP4qAQQrXzrGZUwOcCM5td0/kR840+i8r0eVSmz6Ncuj4LtWhERHKUAl5EJEcp4FNvQtQFZBB9FpXp86hMn0e5tHwW6sGLiOQobcGLiOQoBbyISI5SwKeAmXU1s6lmttDMFpjZuKhrygRmVmBmb5rZs1HXEiUza2tmk81ssZktMrOhUdcUJTP7SeLfyXwze8zMmkddU2Mys0lmtsbM5ld4bF8ze8HMShLX7VKxLgV8amwDrgoh9AGGAJeZWZ+Ia8oE44BFUReRAe4E/hNCOAw4kjz+TMzsQOAKYFAIoS8+lfh3oq2q0f0FOKnKY9cB8RDCoUA8cb/BFPApEEJYHUKYm7i9Af8HfGC0VUXLzLoAo4EHo64lSmbWBogBEwFCCF+GED6PtqrINQVamFlToCWwKuJ6GlUIYTrwaZWHTwceStx+CDgjFetSwKeYmXUHjgJej7aSyN0BXAvsiLqQiPUASoE/J9pVD5pZq6iLikoI4SPgVmA5sBpYF0KYEm1VGWH/EMLqxO3/A/ZPxUIV8ClkZnsD/wSuDCGsj7qeqJjZqcCaEMKcqGvJAE2BAcC9IYSjgE2k6Od3Nkr0lk/Hv/g6A63MbEy0VWWW4GPXUzJ+XQGfImbWDA/3R0MIT0RdT8SGAaeZ2VLg78DxZvZItCVFZiWwMoSQ/EU3GQ/8fPU14MMQQmkIoQx4AvhqxDVlgo/NrBNA4npNKhaqgE8BMzO8x7oohHB71PVELYRwfQihSwihO74DrTiEkJdbaSGE/wNWmFmvxEMjgYURlhS15cAQM2uZ+Hczkjze6VzB08APErd/ADyVioUq4FNjGPA9fEt1XuJyStRFSca4HHjUzN4G+gO/i7ieyCR+yUwG5gLv4BmUV1MWmNljwEygl5mtNLMLgJuBE8ysBP+Vc3NK1qWpCkREcpO24EVEcpQCXkQkRyngRURylAJeRCRHKeBFRHKUAl7ynpl1NrPJEa5/eENn3DSzjRVun2JmS8zsoIZXJ9lMAS+RMdeg/wcTE1Y1SAhhVQjhzIYup67MrKCB76/xbzazkcB44OQQwrKGrEeynwJeGpWZdTezd83sYWA+0NXMrjGzN8zsbTP7dYXX/iLx2lcS84ZfnXh8mpndYWazgXFmNtDMXjKzOWb2fIVDvq9IzNH/tpn9PfHYcRUORnvTzFonapqfeL65mf3ZzN5JPD8i8fh5ZvaEmf0nMWf3H2v4+0Ym3vdOYt7vvRKPLzWzP5jZXODbZnZSYn74ucA3K7y/VeJ9sxLLOb3C+p82s2J8Otnq1h0DHgBODSG835D/TpIbGrz1I1IPhwI/CCG8ZmajEvePBgx4OhFUm4Fv4fOnN8OPfKw4eVlhCGFQYg6gl4DTQwilZnY2cBNwPj6pV48QwlYza5t439XAZSGEVxOTw22pUttl+HxPR5jZYcAUM+uZeK4/PlPoVuBdM7srhLAi+UbzE1f8BRgZQliS+BK7FJ9ZE2BtCGFA4nUlwPHAe8DjFdZ/Az61w/mJmmeZ2YuJ5wYA/UIIVaeaBdgL+F9geAhhcTXPSx7SFrxEYVkI4bXE7VGJy5t4iB+GB/4w4KkQwpbEHPvPVFlGMhR7AX2BF8xsHvBzoEviubfxKQLG4CdlAXgVuN3MrgDahhC2UdmxwCMAiaBcBiQDPh5CWBdC2ILPJ1O1x90Ln0hrSeL+Q/hc8FVrPizxupLEzIEVJ2IbBVyX+FumAc2BbonnXqgh3AHKgBnABTU8L3lIAS9R2FThtgG/DyH0T1wOCSFM3INlGLCgwvuPCCGMSjw3Grgb3/J9w8yahhBuBi4EWgCvJrbS62prhdvb2fNfwJtqfwkGfKvC39MthJCcjGt3798BnAUcbWb/tYd1SY5SwEvUngfOT7RLMLMDzWw/fEv764me+N7AqTW8/12goyXOc2pmzczs8MTO264hhKnAz4A2wN5mdnAI4Z0Qwh+AN/Ct6YpeBs5NLKsnvvX8bh3/lneB7mZ2SOL+9/D2UVWLE687OHH/nArPPQ9cnphpETM7qo7rJoTwBf6ldm5iAivJc+rBS6RCCFPMrDcwM5FpG4ExIYQ3zOxpvM3yMT7z4Lpq3v+lmZ0JjDc/PV5TvOe9BHgk8ZgB40MIn5vZbxM7TncAC4B/A50qLPIe4F4zewdv65yX6OHX5W/ZYmY/BP6RGOnyBnBfDa8bC/zLzL7Av1RaJ57+baL+txNfUh9S85dbdTV8amYnAdPNrDSE8HRd3yu5R7NJSsYys71DCBvNrCUwHRibPPetiNROW/CSySaYWR98R+NDCneRPaMteBGRHKWdrCIiOUoBLyKSoxTwIiI5SgEvIpKjFPAiIjnq/wO56AFez5BwQwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]}]}